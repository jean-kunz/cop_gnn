{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "ic(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x12c44bb10>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Zachary Karate Club graph\n",
    "G = nx.karate_club_graph()\n",
    "num_nodes = G.number_of_nodes()\n",
    "\n",
    "\n",
    "def plot_graph(G, figsize=(10, 7)):\n",
    "    club_dict = nx.get_node_attributes(G, \"club\")\n",
    "    colors = [\n",
    "        \"lightblue\" if club_dict[node] == \"Mr. Hi\" else \"red\" for node in G.nodes()\n",
    "    ]\n",
    "    plt.figure(figsize=figsize)\n",
    "    nx.draw(\n",
    "        G,\n",
    "        pos=nx.spring_layout(G, seed=42),\n",
    "        with_labels=True,\n",
    "        node_color=colors,\n",
    "        edge_color=\"gray\",\n",
    "        cmap=\"Set2\",\n",
    "    )\n",
    "\n",
    "\n",
    "# plot_graph(G, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/biased_rw.png)\n",
    "\n",
    "in get_alias_edge: \n",
    "- src= t\n",
    "- dst= w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alias_edge(G, src, dst, p, q):\n",
    "    \"\"\"Compute transition probabilities for neighbors of 'dst' based on Node2Vec bias.\n",
    "    dst: current node\n",
    "    src: previous node\n",
    "    p: return back to previous node. Lower p means backtracking is more likely.\n",
    "    q: in-out param: explore new nodes (ratio of bfs[breadth-first search] to dfs[depth-first search]). \n",
    "        lower q means more exploration of distant nodes\n",
    "    \"\"\"\n",
    "    unnormalized_probs = []\n",
    "    for neighbor in G[dst]:\n",
    "        if neighbor == src:\n",
    "            weight = 1.0 / p\n",
    "        elif G.has_edge(neighbor, src):  # if neighbor is a neighbor of src (above ex: s1 is neighbor of t)\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            weight = 1.0 / q\n",
    "        unnormalized_probs.append(weight)\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs = [w / norm_const for w in unnormalized_probs]\n",
    "    return list(G[dst]), normalized_probs\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start, walk_length, p, q):\n",
    "    \"\"\"Generate a random walk starting from the given node.\"\"\"\n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        neighbors = list(G.neighbors(cur))\n",
    "        if len(neighbors) == 0:\n",
    "            break\n",
    "        if len(walk) == 1:\n",
    "            next_node = random.choice(neighbors)\n",
    "        else:\n",
    "            prev = walk[-2]\n",
    "            candidates, probs = get_alias_edge(G, src=prev, dst=cur, p=p, q=q)\n",
    "            next_node = random.choices(candidates, weights=probs, k=1)[0]\n",
    "        walk.append(next_node)\n",
    "    return walk\n",
    "\n",
    "\n",
    "nodes = list(G.nodes())\n",
    "node = nodes[0]\n",
    "ic(node)\n",
    "for i in range(10):\n",
    "    walk = node2vec_walk(G, start=node, walk_length=10, p=1, q=1)\n",
    "    ic(walk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decompose walk\n",
    "\n",
    "![](../resources/zachary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "walk = [start]\n",
    "p = 1\n",
    "q = 1\n",
    "for i in range(3):\n",
    "    cur = walk[-1]\n",
    "    neighbors = list(G.neighbors(cur))\n",
    "    ic(neighbors)\n",
    "\n",
    "    if len(neighbors) == 0:\n",
    "        break\n",
    "    if len(walk) == 1:\n",
    "        next_node = random.choice(neighbors)\n",
    "    else:\n",
    "        prev = walk[-2]\n",
    "        candidates, probs = get_alias_edge(G, prev, cur, p, q)\n",
    "        ic(candidates)\n",
    "        ic(probs)\n",
    "        next_node = random.choices(candidates, weights=probs, k=1)[0]\n",
    "    ic(next_node)\n",
    "    walk.append(next_node)\n",
    "    ic(walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random walks for all nodes\n",
    "\n",
    "# Node2Vec parameters\n",
    "p = 1.0\n",
    "q = 1.0\n",
    "walk_length = 10\n",
    "num_walks = 10\n",
    "walks = []\n",
    "for _ in range(num_walks):\n",
    "    nodes = list(G.nodes())\n",
    "    random.shuffle(nodes)\n",
    "    for node in nodes:\n",
    "        walk = node2vec_walk(G, node, walk_length, p, q)\n",
    "        walks.append(walk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[18, 32, 22, 33, 26, 29, 26, 29, 33, 31],\n",
       " [20, 33, 15, 32, 22, 32, 29, 33, 22, 33],\n",
       " [12, 0, 3, 2, 9, 2, 28, 31, 24, 27],\n",
       " [15, 32, 29, 26, 33, 19, 33, 20, 32, 2],\n",
       " [28, 2, 9, 2, 3, 0, 12, 0, 21, 1]]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "walks[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| w: [18, 32, 22, 33, 26, 29, 26, 29, 33, 31]\n",
      "ic| i: 0, target: 18, start: 0, end: 3\n",
      "ic| 'pair', target: 18, walk[j]: 32\n",
      "ic| 'pair', target: 18, walk[j]: 22\n",
      "ic| i: 1, target: 32, start: 0, end: 4\n",
      "ic| 'pair', target: 32, walk[j]: 18\n",
      "ic| 'pair', target: 32, walk[j]: 22\n",
      "ic| 'pair', target: 32, walk[j]: 33\n",
      "ic| i: 2, target: 22, start: 0, end: 5\n",
      "ic| 'pair', target: 22, walk[j]: 18\n",
      "ic| 'pair', target: 22, walk[j]: 32\n",
      "ic| 'pair', target: 22, walk[j]: 33\n",
      "ic| 'pair', target: 22, walk[j]: 26\n",
      "ic| i: 3, target: 33, start: 1, end: 6\n",
      "ic| 'pair', target: 33, walk[j]: 32\n",
      "ic| 'pair', target: 33, walk[j]: 22\n",
      "ic| 'pair', target: 33, walk[j]: 26\n",
      "ic| 'pair', target: 33, walk[j]: 29\n",
      "ic| i: 4, target: 26, start: 2, end: 7\n",
      "ic| 'pair', target: 26, walk[j]: 22\n",
      "ic| 'pair', target: 26, walk[j]: 33\n",
      "ic| 'pair', target: 26, walk[j]: 29\n",
      "ic| 'pair', target: 26, walk[j]: 26\n",
      "ic| i: 5, target: 29, start: 3, end: 8\n",
      "ic| 'pair', target: 29, walk[j]: 33\n",
      "ic| 'pair', target: 29, walk[j]: 26\n",
      "ic| 'pair', target: 29, walk[j]: 26\n",
      "ic| 'pair', target: 29, walk[j]: 29\n",
      "ic| i: 6, target: 26, start: 4, end: 9\n",
      "ic| 'pair', target: 26, walk[j]: 26\n",
      "ic| 'pair', target: 26, walk[j]: 29\n",
      "ic| 'pair', target: 26, walk[j]: 29\n",
      "ic| 'pair', target: 26, walk[j]: 33\n",
      "ic| i: 7, target: 29, start: 5, end: 10\n",
      "ic| 'pair', target: 29, walk[j]: 29\n",
      "ic| 'pair', target: 29, walk[j]: 26\n",
      "ic| 'pair', target: 29, walk[j]: 33\n",
      "ic| 'pair', target: 29, walk[j]: 31\n",
      "ic| i: 8, target: 33, start: 6, end: 10\n",
      "ic| 'pair', target: 33, walk[j]: 26\n",
      "ic| 'pair', target: 33, walk[j]: 29\n",
      "ic| 'pair', target: 33, walk[j]: 31\n",
      "ic| i: 9, target: 31, start: 7, end: 10\n",
      "ic| 'pair', target: 31, walk[j]: 29\n",
      "ic| 'pair', target: 31, walk[j]: 33\n"
     ]
    }
   ],
   "source": [
    "window_size = 2\n",
    "embedding_dim = 16\n",
    "num_negative_samples = 5\n",
    "\n",
    "\n",
    "# A walk represents a sequence of nodes traversed in the graph. \n",
    "# For each node (called target) in a walk, the algorithm creates training pairs \n",
    "# with surrounding nodes within a window of size window_size.\n",
    "# window size = 2 means that for each target node,\n",
    "# the algorithm will consider the two nodes before and two nodes after it in the walk.\n",
    "\n",
    "def gen_walk_pairs(walk, window_size):\n",
    "    walk_pairs = []\n",
    "    for i, target in enumerate(walk):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(walk), i + window_size + 1)\n",
    "        ic(i, target, start, end)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                ic(\"pair\",target, walk[j])\n",
    "                walk_pairs.append((target, walk[j]))\n",
    "\n",
    "    return walk_pairs\n",
    "\n",
    "ic.enable()\n",
    "w = walks[0]\n",
    "ic(w)\n",
    "walk_pairs = gen_walk_pairs(w, window_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.disable()\n",
    "pairs = []\n",
    "for walk in walks:\n",
    "    walk_pairs = gen_walk_pairs(walk, window_size)\n",
    "    pairs.extend(walk_pairs)\n",
    "\n",
    "pairs = np.array(pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/loss_rw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| num_negative_samples: 5\n",
      "ic| negative_batch: tensor([[11, 15,  3, 25, 24],\n",
      "                            [17, 22, 23, 21, 10],\n",
      "                            [26, 12, 22,  5, 20],\n",
      "                            [29, 32,  1,  8, 13],\n",
      "                            [28, 33, 18,  2, 27],\n",
      "                            [17, 21,  1, 33, 18],\n",
      "                            [10, 30, 21, 26,  1],\n",
      "                            [10, 27,  7, 30, 22],\n",
      "                            [29, 27, 14, 28,  4],\n",
      "                            [ 8,  5,  5,  2, 32],\n",
      "                            [ 9, 28,  0,  3,  3],\n",
      "                            [27, 21, 14, 31, 10],\n",
      "                            [26, 16, 17, 23, 30],\n",
      "                            [29,  4, 27, 30, 18],\n",
      "                            [16, 28, 30, 25, 20],\n",
      "                            [ 5, 12, 22, 11, 19],\n",
      "                            [33, 16, 19, 32, 30],\n",
      "                            [20, 28, 20, 11,  8],\n",
      "                            [ 3,  4, 10,  6,  0],\n",
      "                            [ 4,  8, 28,  0, 32],\n",
      "                            [ 7, 20,  7, 10,  3],\n",
      "                            [11,  7, 28,  9, 10],\n",
      "                            [16, 10,  4, 26, 20],\n",
      "                            [18, 11,  7, 10, 13],\n",
      "                            [27,  5, 16,  4, 24],\n",
      "                            [25, 29,  3, 23,  9],\n",
      "                            [22, 14, 10,  2, 26],\n",
      "                            [13, 15, 17, 23, 24],\n",
      "                            [ 0, 13,  6, 30,  9],\n",
      "                            [ 2,  7, 24,  6, 10],\n",
      "                            [33, 12, 24, 29, 13],\n",
      "                            [ 1, 27, 20, 16,  8],\n",
      "                            [18, 31,  3, 24,  7],\n",
      "                            [ 5, 15, 18, 27, 14],\n",
      "                            [ 6,  9, 23, 26,  8],\n",
      "                            [27, 15, 33,  4, 20],\n",
      "                            [32, 24, 26, 29, 23],\n",
      "                            [ 7,  8,  1, 17,  2],\n",
      "                            [ 1, 12, 15, 26, 19],\n",
      "                            [12, 11, 29, 18,  4],\n",
      "                            [10, 20, 33, 20, 31],\n",
      "                            [10,  2, 31, 17, 13],\n",
      "                            [22, 13, 10, 15, 21],\n",
      "                            [ 2,  8, 13, 28, 21],\n",
      "                            [25, 23, 10,  8,  2],\n",
      "                            [20, 30, 15, 10, 20],\n",
      "                            [28, 33,  5,  5, 20],\n",
      "                            [17, 32, 24,  9,  4],\n",
      "                            [11,  4, 27, 28, 18],\n",
      "                            [ 1, 14, 14, 26,  7],\n",
      "                            [11,  8, 19, 32, 33],\n",
      "                            [29,  5, 20, 22, 30],\n",
      "                            [22, 28,  0,  2, 19],\n",
      "                            [21, 21, 17, 26, 17],\n",
      "                            [29,  0, 10, 23, 33],\n",
      "                            [ 5, 17, 28, 12,  0],\n",
      "                            [ 8, 22, 15, 21, 19],\n",
      "                            [ 2,  8,  7, 29,  6],\n",
      "                            [21,  3, 29,  6,  1],\n",
      "                            [10, 24, 21, 21,  1],\n",
      "                            [ 7, 25,  3, 15, 20],\n",
      "                            [20, 23,  2,  3, 10],\n",
      "                            [10, 25, 21,  5,  2],\n",
      "                            [25, 16, 19,  8, 21]], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "ic.enable()\n",
    "ic(num_negative_samples)\n",
    "negative_batch = torch.LongTensor(\n",
    "    np.random.randint(0, num_nodes, size=(len(batch), num_negative_samples))\n",
    ").to(device)\n",
    "ic(negative_batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| target.shape: torch.Size([2])\n",
      "    context.shape: torch.Size([2])\n",
      "    negative.shape: torch.Size([2, 2])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(2.1213, device='mps:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the Node2Vec Skip-Gram model\n",
    "class Node2Vec(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim):\n",
    "        super(Node2Vec, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(num_nodes, embedding_dim)\n",
    "        self.out_embeddings = nn.Embedding(num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.in_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.out_embeddings.weight)\n",
    "\n",
    "    def forward(self, target, context, negative):\n",
    "        ic(target.shape, context.shape, negative.shape)\n",
    "        embed_target = self.in_embeddings(target)  # (batch, embedding_dim)\n",
    "        embed_context = self.out_embeddings(context)  # (batch, embedding_dim)\n",
    "        score = torch.sum(embed_target * embed_context, dim=1)\n",
    "        log_target = torch.log(torch.sigmoid(score) + 1e-10)\n",
    "\n",
    "        embed_negative = self.out_embeddings(\n",
    "            negative\n",
    "        )  # (batch, num_negative, embedding_dim)\n",
    "        neg_score = torch.bmm(embed_negative, embed_target.unsqueeze(2)).squeeze()\n",
    "        log_negative = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-10), dim=1)\n",
    "\n",
    "        loss = -(log_target + log_negative)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "model = Node2Vec(num_nodes, embedding_dim).to(device)\n",
    "\n",
    "model.forward(target=torch.LongTensor([0,1]).to(device), \n",
    "      context=torch.LongTensor([1,2]).to(device),\n",
    "      negative=torch.LongTensor([[3,2],[4,6]]).to(device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 1.9572\n",
      "Epoch 20, Loss: 1.9548\n",
      "Epoch 30, Loss: 1.9507\n",
      "Epoch 40, Loss: 1.9668\n",
      "Epoch 50, Loss: 1.9616\n",
      "Epoch 60, Loss: 1.9580\n",
      "Epoch 70, Loss: 1.9565\n",
      "Epoch 80, Loss: 1.9482\n",
      "Epoch 90, Loss: 1.9579\n",
      "Epoch 100, Loss: 1.9583\n"
     ]
    }
   ],
   "source": [
    "model = Node2Vec(num_nodes, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "num_batches = len(pairs) // batch_size + 1\n",
    "ic.disable()\n",
    "for epoch in range(num_epochs):\n",
    "    np.random.shuffle(pairs)\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i : i + batch_size]\n",
    "        ic(batch.shape,batch[:,0].shape)\n",
    "        target_batch = torch.LongTensor(batch[:, 0]).to(device)\n",
    "        context_batch = torch.LongTensor(batch[:, 1]).to(device)\n",
    "        # Nb: there is no guarantee that the negative samples are actually negative (not connected to target nodes)\n",
    "        # to be perfectly negative, For each target node, sample from nodes that are not its neighbors\n",
    "        negative_batch = torch.LongTensor(\n",
    "            np.random.randint(0, num_nodes, size=(len(batch), num_negative_samples))\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(target_batch, context_batch, negative_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / num_batches:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Retrieve learned embeddings\n",
    "embeddings = model.in_embeddings.weight.data.cpu().numpy()\n",
    "print(\"\\nLearned embeddings:\")\n",
    "for node in range(num_nodes):\n",
    "    print(f\"Node {node}: {embeddings[node]}\")\n",
    "\n",
    "# Use PCA to reduce the embeddings to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create colors list based on club membership: red for 'Mr. Hi', blue for others.\n",
    "colors = []\n",
    "for node in range(num_nodes):\n",
    "    club = G.nodes[node][\"club\"]\n",
    "    colors.append(\"red\" if club == \"Mr. Hi\" else \"blue\")\n",
    "\n",
    "# Plotting the PCA visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors)\n",
    "for i in range(num_nodes):\n",
    "    plt.annotate(str(i), (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=9)\n",
    "plt.title(\"PCA Visualization of Node2Vec Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gemini\n",
    "\n",
    "Solution suggested by gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load the Zachary Karate Club graph\n",
    "def load_karate_graph():\n",
    "    \"\"\"\n",
    "    Loads the Zachary Karate Club graph from NetworkX.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: The Karate Club graph.\n",
    "    \"\"\"\n",
    "    G = nx.karate_club_graph()\n",
    "    return G\n",
    "\n",
    "\n",
    "# Convert NetworkX graph to adjacency list\n",
    "def graph_to_adj_list(graph):\n",
    "    \"\"\"\n",
    "    Converts a NetworkX graph to an adjacency list representation.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: An adjacency list where keys are nodes and values are lists of neighbors.\n",
    "    \"\"\"\n",
    "    adj_list = defaultdict(list)\n",
    "    for node in graph.nodes():\n",
    "        adj_list[node] = list(graph.neighbors(node))\n",
    "    return adj_list\n",
    "\n",
    "\n",
    "# Generate random walk\n",
    "def generate_random_walk(graph, start_node, walk_length, p, q):\n",
    "    \"\"\"\n",
    "    Generates a random walk starting from a given node.  This implementation\n",
    "    correctly handles the edge weights (p and q) for the walk.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        start_node (int): The starting node for the random walk.\n",
    "        walk_length (int): The length of the random walk.\n",
    "        p (float): The return parameter.\n",
    "        q (float): The in-out parameter.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of nodes representing the random walk.\n",
    "    \"\"\"\n",
    "    walk = [start_node]\n",
    "    for _ in range(walk_length - 1):\n",
    "        current_node = walk[-1]\n",
    "        neighbors = list(graph.neighbors(current_node))\n",
    "        if not neighbors:\n",
    "            break  # Handle disconnected nodes\n",
    "        if len(walk) == 1:\n",
    "            # Start node case: sample directly from neighbors\n",
    "            next_node = random.choice(neighbors)\n",
    "            walk.append(next_node)\n",
    "        else:\n",
    "            # Subsequent step case: consider p and q\n",
    "            previous_node = walk[-2]\n",
    "            probabilities = []\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor == previous_node:\n",
    "                    probabilities.append(1 / p)  # Return probability\n",
    "                elif graph.has_edge(previous_node, neighbor):\n",
    "                    probabilities.append(1)  # Neighboring node\n",
    "                else:\n",
    "                    probabilities.append(1 / q)  # Non-neighboring node\n",
    "            # Normalize probabilities to sum to 1\n",
    "            probabilities = [prob / sum(probabilities) for prob in probabilities]\n",
    "            # Use the probabilities to select the next node.\n",
    "            next_node = random.choices(neighbors, probabilities)[0]\n",
    "            walk.append(next_node)\n",
    "    return walk\n",
    "\n",
    "\n",
    "# Generate multiple random walks\n",
    "def generate_walks(graph, num_walks, walk_length, p, q):\n",
    "    \"\"\"\n",
    "    Generates multiple random walks for each node in the graph.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        num_walks (int): The number of random walks per node.\n",
    "        walk_length (int): The length of each random walk.\n",
    "        p (float): The return parameter.\n",
    "        q (float): The in-out parameter.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each inner list is a random walk.\n",
    "    \"\"\"\n",
    "    walks = []\n",
    "    nodes = list(graph.nodes())\n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(nodes)  # Shuffle for each iteration\n",
    "        for node in nodes:\n",
    "            walk = generate_random_walk(graph, node, walk_length, p, q)\n",
    "            walks.append(walk)\n",
    "    return walks\n",
    "\n",
    "\n",
    "# Skip-gram model with negative sampling\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-gram model for learning node embeddings.  This implementation\n",
    "    uses PyTorch and includes proper initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_nodes, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initializes the Skip-gram model.\n",
    "\n",
    "        Args:\n",
    "            num_nodes (int): The number of nodes in the graph.\n",
    "            embedding_dim (int): The dimensionality of the node embeddings.\n",
    "        \"\"\"\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embedding_u = torch.nn.Embedding(\n",
    "            num_nodes, embedding_dim\n",
    "        )  # Target embeddings\n",
    "        self.embedding_v = torch.nn.Embedding(\n",
    "            num_nodes, embedding_dim\n",
    "        )  # Context embeddings\n",
    "\n",
    "        # Initialize embeddings using a small uniform distribution\n",
    "        init_range = 0.5 / embedding_dim\n",
    "        self.embedding_u.weight.data.uniform_(-init_range, init_range)\n",
    "        self.embedding_v.weight.data.uniform_(\n",
    "            -0, 0\n",
    "        )  # init_range, init_range) # Initializing to 0 can sometimes help\n",
    "\n",
    "    def forward(self, center_nodes, context_nodes, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the Skip-gram model.\n",
    "\n",
    "        Args:\n",
    "            center_nodes (torch.Tensor): Tensor of center node indices.\n",
    "            context_nodes (torch.Tensor): Tensor of context node indices.\n",
    "            negative_samples (torch.Tensor): Tensor of negative sample indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The loss value.\n",
    "        \"\"\"\n",
    "        # Get embeddings for center nodes\n",
    "        u_embeddings = self.embedding_u(center_nodes)  # (batch_size, embedding_dim)\n",
    "\n",
    "        # Get embeddings for context nodes\n",
    "        v_embeddings = self.embedding_v(context_nodes)  # (batch_size, embedding_dim)\n",
    "\n",
    "        # Compute the positive log-likelihood\n",
    "        positive_scores = torch.sum(u_embeddings * v_embeddings, dim=1)  # (batch_size,)\n",
    "        positive_loss = -torch.mean(torch.log(torch.sigmoid(positive_scores)))\n",
    "\n",
    "        # Get embeddings for negative samples\n",
    "        negative_embeddings = self.embedding_v(\n",
    "            negative_samples\n",
    "        )  # (batch_size, num_negative_samples, embedding_dim)\n",
    "\n",
    "        # Compute the negative log-likelihood\n",
    "        negative_scores = torch.bmm(\n",
    "            negative_embeddings, u_embeddings.unsqueeze(2)\n",
    "        ).squeeze(2)  # (batch_size, num_negative_samples)\n",
    "        negative_loss = -torch.mean(torch.log(torch.sigmoid(-negative_scores)))\n",
    "\n",
    "        # Combine positive and negative losses\n",
    "        loss = positive_loss + negative_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "def generate_negative_samples(walks, num_nodes, window_size, num_negative_samples):\n",
    "    \"\"\"\n",
    "    Generates negative samples for each center node and context node pair.\n",
    "    This version pre-computes the unigram distribution and samples directly\n",
    "    according to that distribution.\n",
    "\n",
    "    Args:\n",
    "        walks (list): A list of random walks.\n",
    "        num_nodes (int): The total number of nodes in the graph.\n",
    "        window_size (int): The context window size.\n",
    "        num_negative_samples (int): The number of negative samples per (center, context) pair.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "            (center_node, context_node, list of negative samples)\n",
    "    \"\"\"\n",
    "    negative_samples = []\n",
    "    node_counts = [0] * num_nodes  # Initialize node counts\n",
    "    for walk in walks:\n",
    "        for node in walk:\n",
    "            node_counts[node] += 1\n",
    "\n",
    "    # Compute the unigram distribution raised to the power of 0.75\n",
    "    total_count = sum(node_counts)\n",
    "    unigram_dist = [(count / total_count) ** 0.75 for count in node_counts]\n",
    "    # Normalize the distribution\n",
    "    unigram_dist = [p / sum(unigram_dist) for p in unigram_dist]\n",
    "\n",
    "    for walk in walks:\n",
    "        for i, center_node in enumerate(walk):\n",
    "            for j in range(\n",
    "                max(0, i - window_size), min(len(walk), i + window_size + 1)\n",
    "            ):\n",
    "                if i != j:\n",
    "                    context_node = walk[j]\n",
    "                    # Generate negative samples, ensuring they are different from the context node\n",
    "                    neg_samples = random.choices(\n",
    "                        range(num_nodes), weights=unigram_dist, k=num_negative_samples\n",
    "                    )\n",
    "                    negative_samples.append((center_node, context_node, neg_samples))\n",
    "    return negative_samples\n",
    "\n",
    "\n",
    "def train_node2vec(\n",
    "    graph,\n",
    "    walks,\n",
    "    embedding_dim,\n",
    "    window_size,\n",
    "    num_negative_samples,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    p,\n",
    "    q,\n",
    "    num_workers=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the Node2vec model. This function incorporates the negative sampling\n",
    "    and training loop, and uses the graph, walks, and p, q parameters.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        walks (list): A list of random walks.\n",
    "        embedding_dim (int): The dimensionality of the node embeddings.\n",
    "        window_size (int): The context window size.\n",
    "        num_negative_samples (int): The number of negative samples per (center, context) pair.\n",
    "        batch_size (int): The batch size for training.\n",
    "        epochs (int): The number of training epochs.\n",
    "        learning_rate (float): The learning rate.\n",
    "        p (float): The return parameter.\n",
    "        q (float): The in-out parameter.\n",
    "        num_workers (int): Number of workers for data loading (not used in this basic implementation).\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained Skip-gram model.\n",
    "    \"\"\"\n",
    "    # Determine the device to use\n",
    "    device = (\n",
    "        torch.device(\"mps\")\n",
    "        if torch.backends.mps.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    model = SkipGramModel(num_nodes, embedding_dim).to(device)  # Move model to device\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Use Binary Cross Entropy Loss (though we calculate it manually for more control)\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss() # Not used, but kept here for reference\n",
    "\n",
    "    negative_samples = generate_negative_samples(\n",
    "        walks, num_nodes, window_size, num_negative_samples\n",
    "    )\n",
    "\n",
    "    # Prepare data for PyTorch (create tensors) outside the loop\n",
    "    center_nodes_all = []\n",
    "    context_nodes_all = []\n",
    "    negative_samples_all = []\n",
    "\n",
    "    for center_node, context_node, neg_samples in negative_samples:\n",
    "        center_nodes_all.append(center_node)\n",
    "        context_nodes_all.append(context_node)\n",
    "        negative_samples_all.append(neg_samples)\n",
    "\n",
    "    center_nodes_tensor = torch.tensor(center_nodes_all, dtype=torch.long).to(\n",
    "        device\n",
    "    )  # Move tensors to device\n",
    "    context_nodes_tensor = torch.tensor(context_nodes_all, dtype=torch.long).to(device)\n",
    "    negative_samples_tensor = torch.tensor(negative_samples_all, dtype=torch.long).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    num_samples = len(negative_samples)\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the training data at the beginning of each epoch\n",
    "        permutation = torch.randperm(num_samples).to(\n",
    "            device\n",
    "        )  # Move permutation to device\n",
    "        center_nodes_tensor = center_nodes_tensor[permutation]\n",
    "        context_nodes_tensor = context_nodes_tensor[permutation]\n",
    "        negative_samples_tensor = negative_samples_tensor[permutation]\n",
    "        total_loss = 0.0\n",
    "        for i in tqdm(\n",
    "            range(0, num_samples, batch_size), desc=f\"Epoch {epoch + 1}/{epochs}\"\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            # Get batch\n",
    "            indices = torch.arange(i, min(i + batch_size, num_samples)).to(\n",
    "                device\n",
    "            )  # Move indices to device\n",
    "            batch_center_nodes = center_nodes_tensor[indices]\n",
    "            batch_context_nodes = context_nodes_tensor[indices]\n",
    "            batch_negative_samples = negative_samples_tensor[indices]\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(\n",
    "                batch_center_nodes, batch_context_nodes, batch_negative_samples\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / (num_samples // batch_size)}\"\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_node_embeddings(model):\n",
    "    \"\"\"\n",
    "    Gets the node embeddings from the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained Skip-gram model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the node embeddings.\n",
    "    \"\"\"\n",
    "    return model.embedding_u.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def visualize_embeddings(embeddings, graph, title=\"Node2vec Embeddings\"):\n",
    "    \"\"\"\n",
    "    Visualizes the node embeddings using a 2D scatter plot.  This version\n",
    "    uses the actual node labels from the Karate Club graph.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): The node embeddings.\n",
    "        graph (nx.Graph): The input graph (for getting node labels).\n",
    "        title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    # Use a simple PCA for dimensionality reduction to 2D\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Get the club labels for coloring\n",
    "    club_labels = [graph.nodes[node][\"club\"] for node in graph.nodes()]\n",
    "    colors = [\"red\" if label == \"Mr. Hi\" else \"blue\" for label in club_labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=colors)\n",
    "\n",
    "    # Add node labels to the plot\n",
    "    for i, node in enumerate(graph.nodes()):\n",
    "        plt.annotate(\n",
    "            str(node),\n",
    "            xy=(reduced_embeddings[i, 0], reduced_embeddings[i, 1]),\n",
    "            xytext=(5, 2),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    embedding_dim = 128\n",
    "    walk_length = 20\n",
    "    num_walks = 10\n",
    "    window_size = 5\n",
    "    num_negative_samples = 5\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    learning_rate = 0.05  # Increased learning rate\n",
    "    p = 1.0\n",
    "    q = 1.0\n",
    "\n",
    "    # Load graph\n",
    "    karate_graph = load_karate_graph()\n",
    "\n",
    "    # Generate random walks\n",
    "    walks = generate_walks(karate_graph, num_walks, walk_length, p, q)\n",
    "\n",
    "    # Train Node2vec model\n",
    "    model = train_node2vec(\n",
    "        karate_graph,\n",
    "        walks,\n",
    "        embedding_dim,\n",
    "        window_size,\n",
    "        num_negative_samples,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        learning_rate,\n",
    "        p,\n",
    "        q,\n",
    "    )\n",
    "\n",
    "    # Get node embeddings\n",
    "    node_embeddings = get_node_embeddings(model)\n",
    "\n",
    "    # Visualize embeddings\n",
    "    visualize_embeddings(\n",
    "        node_embeddings, karate_graph, title=\"Node2vec Embeddings (Karate Club)\"\n",
    "    )\n",
    "\n",
    "    # Print the first 5 embeddings\n",
    "    print(\"First 5 Node Embeddings:\")\n",
    "    print(node_embeddings[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
