{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2vec\n",
    "\n",
    "https://arxiv.org/abs/1607.00653\n",
    "\n",
    "https://pytorch-geometric.readthedocs.io/en/latest/tutorial/shallow_node_embeddings.html\n",
    "\n",
    "https://spotintelligence.com/2024/01/18/node2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from icecream import ic\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "ic(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seeds for reproducibility\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Zachary Karate Club graph\n",
    "G = nx.karate_club_graph()\n",
    "num_nodes = G.number_of_nodes()\n",
    "\n",
    "\n",
    "def plot_graph(G, figsize=(10, 7)):\n",
    "    club_dict = nx.get_node_attributes(G, \"club\")\n",
    "    colors = [\n",
    "        \"lightblue\" if club_dict[node] == \"Mr. Hi\" else \"red\" for node in G.nodes()\n",
    "    ]\n",
    "    plt.figure(figsize=figsize)\n",
    "    nx.draw(\n",
    "        G,\n",
    "        pos=nx.spring_layout(G, seed=42),\n",
    "        with_labels=True,\n",
    "        node_color=colors,\n",
    "        edge_color=\"gray\",\n",
    "        cmap=\"Set2\",\n",
    "    )\n",
    "\n",
    "\n",
    "# plot_graph(G, figsize=(5, 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/biased_rw.png)\n",
    "\n",
    "in get_alias_edge: \n",
    "- src= t\n",
    "- dst= w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alias_edge(G, src, curr, p, q):\n",
    "    \"\"\"Compute transition probabilities for neighbors of 'dst' based on Node2Vec bias.\n",
    "    dst: current node\n",
    "    src: previous node\n",
    "    p: return back to previous node. Lower p means backtracking is more likely.\n",
    "    q: in-out param: explore new nodes (ratio of bfs[breadth-first search] to dfs[depth-first search]).\n",
    "        lower q means more exploration of distant nodes\n",
    "    \"\"\"\n",
    "    unnormalized_probs = []\n",
    "    for neighbor in G[curr]:\n",
    "        if neighbor == src:\n",
    "            weight = 1.0 / p\n",
    "        elif G.has_edge(\n",
    "            neighbor, src\n",
    "        ):  # if neighbor is a neighbor of src (above ex: s1 is neighbor of t)\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            weight = 1.0 / q\n",
    "        unnormalized_probs.append(weight)\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs = [w / norm_const for w in unnormalized_probs]\n",
    "    return list(G[curr]), normalized_probs\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start, walk_length, p, q):\n",
    "    \"\"\"Generate a random walk starting from the given node.\"\"\"\n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        neighbors = list(G.neighbors(cur))\n",
    "        if len(neighbors) == 0:\n",
    "            break\n",
    "        if len(walk) == 1:\n",
    "            next_node = random.choice(neighbors)\n",
    "        else:\n",
    "            prev = walk[-2]\n",
    "            candidates, probs = get_alias_edge(G, src=prev, curr=cur, p=p, q=q)\n",
    "            next_node = random.choices(candidates, weights=probs, k=1)[0]\n",
    "        walk.append(next_node)\n",
    "    return walk\n",
    "\n",
    "\n",
    "ic.enable()\n",
    "nodes = list(G.nodes())\n",
    "node = nodes[0]\n",
    "ic(node)\n",
    "for i in range(10):\n",
    "    walk = node2vec_walk(G, start=node, walk_length=10, p=1, q=1)\n",
    "    ic(walk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# decompose walk\n",
    "\n",
    "![](../resources/zachary.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "walk = [start]\n",
    "p = 1\n",
    "q = 0.5\n",
    "walk_length = 2\n",
    "for i in range(walk_length):\n",
    "    cur = walk[-1]\n",
    "    neighbors = list(G.neighbors(cur))\n",
    "    ic(len(neighbors))\n",
    "    ic(neighbors)\n",
    "\n",
    "    if len(neighbors) == 0:\n",
    "        break\n",
    "    if len(walk) == 1:\n",
    "        next_node = random.choice(neighbors)\n",
    "    else:\n",
    "        prev = walk[-2]\n",
    "        candidates, probs = get_alias_edge(G, prev, cur, p, q)\n",
    "        ic(candidates)\n",
    "        ic(probs)\n",
    "        next_node = random.choices(candidates, weights=probs, k=1)[0]\n",
    "    ic(next_node)\n",
    "    walk.append(next_node)\n",
    "    ic(walk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random walks for all nodes\n",
    "\n",
    "# Node2Vec parameters\n",
    "p = 1\n",
    "q = 0.2\n",
    "walk_length = 8\n",
    "num_walks = 20\n",
    "walks = []\n",
    "for _ in range(num_walks):\n",
    "    nodes = list(G.nodes())\n",
    "    random.shuffle(nodes)\n",
    "    for node in nodes:\n",
    "        walk = node2vec_walk(G, node, walk_length, p, q)\n",
    "        walks.append(walk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(walks[:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 2\n",
    "embedding_dim = 16\n",
    "num_negative_samples = 5\n",
    "\n",
    "\n",
    "# A walk represents a sequence of nodes traversed in the graph.\n",
    "# For each node (called target) in a walk, the algorithm creates training pairs\n",
    "# with surrounding nodes within a window of size window_size.\n",
    "# window size = 2 means that for each target node,\n",
    "# the algorithm will consider the two nodes before and two nodes after it in the walk.\n",
    "\n",
    "\n",
    "def gen_walk_pairs(walk, window_size):\n",
    "    walk_pairs = []\n",
    "    for i, target in enumerate(walk):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(walk), i + window_size + 1)\n",
    "        ic(i, target, start, end)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                ic(\"pair\", target, j, walk[j])\n",
    "                if walk[j] != target:  # Avoid pairs of the same node\n",
    "                    walk_pairs.append((target, walk[j]))\n",
    "\n",
    "    return walk_pairs\n",
    "\n",
    "\n",
    "ic.enable()\n",
    "w = walks[0]\n",
    "ic(window_size)\n",
    "w = [0, 2, 3, 2, 5, 2, 5]\n",
    "ic(w)\n",
    "walk_pairs = gen_walk_pairs(w, window_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "walk_pairs[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.disable()\n",
    "pairs = []\n",
    "for walk in walks:\n",
    "    walk_pairs = gen_walk_pairs(walk, window_size)\n",
    "    pairs.extend(walk_pairs)\n",
    "\n",
    "pairs = np.array(pairs)\n",
    "len(pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "# Compute pair occurrences\n",
    "pair_counts = Counter(tuple(pair) for pair in pairs)\n",
    "pair_counts\n",
    "# Display the results\n",
    "print(f\"Total number of pairs: {len(pairs)}\")\n",
    "print(f\"Number of unique pairs: {len(pair_counts)}\")\n",
    "print(\"\\nTop 10 most frequent pairs:\")\n",
    "for pair, count in pair_counts.most_common(10):\n",
    "    print(f\"Pair {pair}: {count} occurrences\")\n",
    "\n",
    "# You can also create a dictionary for easy lookup\n",
    "pair_dict = dict(pair_counts)\n",
    "print(f\"\\nExample lookups:\")\n",
    "print(f\"Pair (0, 1) occurs {pair_dict.get((0, 1), 0)} times\")\n",
    "print(f\"Pair (1, 0) occurs {pair_dict.get((1, 0), 0)} times\")\n",
    "\n",
    "# Distribution of pair frequencies\n",
    "freq_distribution = Counter(pair_counts.values())\n",
    "print(f\"\\nFrequency distribution:\")\n",
    "for freq, count in sorted(freq_distribution.items()):\n",
    "    print(f\"{count} pairs occur {freq} time(s)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_dict.get((8, 9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](../resources/loss_rw.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.enable()\n",
    "ic(num_negative_samples)\n",
    "batch_size = 2\n",
    "negative_batch = torch.LongTensor(\n",
    "    np.random.randint(0, num_nodes, size=(batch_size, num_negative_samples))\n",
    ").to(device)\n",
    "ic(negative_batch);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Node2Vec Skip-Gram model\n",
    "class Node2Vec(nn.Module):\n",
    "    def __init__(self, num_nodes, embedding_dim):\n",
    "        super(Node2Vec, self).__init__()\n",
    "        self.in_embeddings = nn.Embedding(num_nodes, embedding_dim)\n",
    "        self.out_embeddings = nn.Embedding(num_nodes, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.in_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.out_embeddings.weight)\n",
    "\n",
    "    def forward(self, target, context, negative):\n",
    "        ic(target.shape, context.shape, negative.shape)\n",
    "        embed_target = self.in_embeddings(target)  # (batch, embedding_dim)\n",
    "        embed_context = self.out_embeddings(context)  # (batch, embedding_dim)\n",
    "        ic(embed_target.shape, embed_context.shape)\n",
    "        score = torch.sum(embed_target * embed_context, dim=1)\n",
    "        ic(score)\n",
    "        log_target = torch.log(torch.sigmoid(score) + 1e-10)\n",
    "        ic(log_target)\n",
    "\n",
    "        embed_negative = self.out_embeddings(\n",
    "            negative\n",
    "        )  # (batch, num_negative, embedding_dim)\n",
    "        ic(embed_negative.shape)\n",
    "        # we could use matmul, but bmm has been specially optimized for batch processing\n",
    "        ic(torch.bmm(embed_negative, embed_target.unsqueeze(2)))\n",
    "        neg_score = torch.bmm(embed_negative, embed_target.unsqueeze(2)).squeeze()\n",
    "        log_negative = torch.sum(torch.log(torch.sigmoid(-neg_score) + 1e-10), dim=1)\n",
    "\n",
    "        loss = -(log_target + log_negative)\n",
    "        ic(loss)\n",
    "        return loss.mean()\n",
    "\n",
    "\n",
    "ic(embedding_dim, num_nodes)\n",
    "model = Node2Vec(num_nodes, embedding_dim).to(device)\n",
    "\n",
    "model.forward(\n",
    "    target=torch.LongTensor([0, 1]).to(device),\n",
    "    context=torch.LongTensor([1, 2]).to(device),\n",
    "    negative=torch.LongTensor([[3, 2, 4], [4, 6, 5]]).to(device),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Characterize the Spatial Properties of embeddings\n",
    "\n",
    "https://medium.com/data-science-collective/whats-happening-to-embeddings-during-training-338c420705e5\n",
    "\n",
    "To start the study, we first need some indicators or metrics that characterize the spatial properties of embeddings. Among the limited literature, the following measures were selected:\n",
    "\n",
    "- Gini Index: Measures inequality of values, i.e., if the information in the vector is concentrated in a few dimensions.\n",
    "- Vector Entropy: Measures distributional uncertainty that reveals how uniformly the embedding uses its dimensions.\n",
    "- Hoyer Sparsity: Measures how many dimensions are effectively utilized.\n",
    "- Spectral Entropy: Gives a frequency-domain view that reflects how smooth or noisy the embedding is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def gini(x):\n",
    "    \"\"\"Compute the Gini coefficient of a vector.\"\"\"\n",
    "    x = np.abs(x.flatten()) + 1e-12  # Avoid division by zero\n",
    "    x_sorted = np.sort(x)\n",
    "    n = len(x)\n",
    "    cumulative = np.cumsum(x_sorted)\n",
    "    gini_coeff = (n + 1 - 2 * np.sum(cumulative) / cumulative[-1]) / n\n",
    "    return gini_coeff\n",
    "\n",
    "\n",
    "def vector_entropy(x):\n",
    "    \"\"\"Compute entropy of normalized absolute vector components.\"\"\"\n",
    "    x = np.abs(x.flatten()) + 1e-12\n",
    "    p = x / np.sum(x)\n",
    "    return -np.sum(p * np.log(p))\n",
    "\n",
    "\n",
    "def hoyer_sparsity(x):\n",
    "    \"\"\"Compute Hoyer's sparsity measure of a vector.\"\"\"\n",
    "    x = np.abs(x.flatten())\n",
    "    n = len(x)\n",
    "    l1 = np.sum(x)\n",
    "    l2 = np.sqrt(np.sum(x**2))\n",
    "    if l1 == 0:\n",
    "        return 0.0\n",
    "    return (np.sqrt(n) - (l1 / l2)) / (np.sqrt(n) - 1)\n",
    "\n",
    "\n",
    "def spectral_entropy(E):\n",
    "    \"\"\"Compute spectral entropy from an embedding matrix E (rows = vectors).\"\"\"\n",
    "    # Compute covariance matrix\n",
    "    cov = np.cov(E, rowvar=False)\n",
    "    # Compute eigenvalues\n",
    "    eigvals = np.linalg.eigvalsh(cov)\n",
    "    eigvals = np.clip(eigvals, a_min=1e-12, a_max=None)  # avoid log(0)\n",
    "    p = eigvals / np.sum(eigvals)\n",
    "    return -np.sum(p * np.log(p))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Node2Vec(num_nodes, embedding_dim).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "batch_size = 64\n",
    "num_batches = len(pairs) // batch_size + 1\n",
    "ic.disable()\n",
    "for epoch in range(num_epochs):\n",
    "    np.random.shuffle(pairs)\n",
    "    epoch_loss = 0\n",
    "    for i in range(0, len(pairs), batch_size):\n",
    "        batch = pairs[i : i + batch_size]\n",
    "        ic(batch.shape, batch[:, 0].shape)\n",
    "        target_batch = torch.LongTensor(batch[:, 0]).to(device)\n",
    "        context_batch = torch.LongTensor(batch[:, 1]).to(device)\n",
    "        # Nb: there is no guarantee that the negative samples are actually negative (not connected to target nodes)\n",
    "        # to be perfectly negative, For each target node, sample from nodes that are not its neighbors.\n",
    "        # Not perfect by still works (just noiser.)\n",
    "        negative_batch = torch.LongTensor(\n",
    "            np.random.randint(0, num_nodes, size=(len(batch), num_negative_samples))\n",
    "        ).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(target_batch, context_batch, negative_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss / num_batches:.4f}\")\n",
    "        embeddings = model.in_embeddings.weight.data.cpu().numpy()\n",
    "        gini_score = round(float(np.mean(list(map(gini, embeddings)))), 4)\n",
    "        hoyer_sparsity_score = round(\n",
    "            float(np.mean(list(map(hoyer_sparsity, embeddings)))), 4\n",
    "        )\n",
    "        vector_entropy_score = round(\n",
    "            float(np.mean(list(map(vector_entropy, embeddings)))), 4\n",
    "        )\n",
    "        spectral_entropy_score = round(float(spectral_entropy(embeddings)), 4)\n",
    "        print(\n",
    "            f\"Gini: {gini_score}, Hoyer's Sparsity: {hoyer_sparsity_score}, Vector Entropy: {vector_entropy_score}, Spectral Entropy: {spectral_entropy_score}\"\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve learned embeddings\n",
    "embeddings = .weight.data.cpu().numpy()\n",
    "print(\"\\nLearned embeddings:\")\n",
    "for node in range(num_nodes):\n",
    "    print(f\"Node {node}: {embeddings[node]}\")\n",
    "\n",
    "# Use PCA to reduce the embeddings to 2D for visualization\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "# Create colors list based on club membership: red for 'Mr. Hi', blue for others.\n",
    "colors = []\n",
    "for node in range(num_nodes):\n",
    "    club = G.nodes[node][\"club\"]\n",
    "    colors.append(\"red\" if club == \"Mr. Hi\" else \"blue\")\n",
    "\n",
    "# Plotting the PCA visualization\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors)\n",
    "for i in range(num_nodes):\n",
    "    plt.annotate(str(i), (embeddings_2d[i, 0], embeddings_2d[i, 1]), fontsize=9)\n",
    "plt.title(\"PCA Visualization of Node2Vec Embeddings\")\n",
    "plt.xlabel(\"Principal Component 1\")\n",
    "plt.ylabel(\"Principal Component 2\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifier on node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def create_edge_features(embeddings, edge_list, method=\"hadamard\"):\n",
    "    \"\"\"\n",
    "    Create edge features from node embeddings using different combination methods.\n",
    "\n",
    "    Args:\n",
    "        embeddings: Node embeddings matrix (num_nodes, embedding_dim)\n",
    "        edge_list: List of edges [(node1, node2), ...]\n",
    "        method: Method to combine embeddings ('hadamard', 'concat', 'l1', 'l2', 'average')\n",
    "\n",
    "    Returns:\n",
    "        Feature matrix for edges\n",
    "    \"\"\"\n",
    "    features = []\n",
    "\n",
    "    for node1, node2 in edge_list:\n",
    "        emb1 = embeddings[node1]\n",
    "        emb2 = embeddings[node2]\n",
    "\n",
    "        if method == \"hadamard\":\n",
    "            # Element-wise multiplication (Hadamard product)\n",
    "            feature = emb1 * emb2\n",
    "        elif method == \"concat\":\n",
    "            # Concatenation\n",
    "            feature = np.concatenate([emb1, emb2])\n",
    "        elif method == \"l1\":\n",
    "            # L1 distance (absolute difference)\n",
    "            feature = np.abs(emb1 - emb2)\n",
    "        elif method == \"l2\":\n",
    "            # L2 distance (squared difference)\n",
    "            feature = (emb1 - emb2) ** 2\n",
    "        elif method == \"average\":\n",
    "            # Average of embeddings\n",
    "            feature = (emb1 + emb2) / 2\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    return np.array(features)\n",
    "\n",
    "\n",
    "def prepare_link_prediction_data(G, embeddings, test_ratio=0.2, neg_pos_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Prepare data for link prediction task.\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX graph\n",
    "        embeddings: Node embeddings\n",
    "        test_ratio: Ratio of edges to use for testing\n",
    "        neg_pos_ratio: Ratio of negative to positive samples\n",
    "\n",
    "    Returns:\n",
    "        Train and test data with features and labels\n",
    "    \"\"\"\n",
    "    # Get all edges (positive examples)\n",
    "    edges = list(G.edges())\n",
    "\n",
    "    # Split edges into train and test\n",
    "    train_edges, test_edges = train_test_split(\n",
    "        edges, test_size=test_ratio, random_state=42\n",
    "    )\n",
    "\n",
    "    # Generate negative examples (non-existing edges)\n",
    "    def generate_negative_edges(G, num_neg_edges, existing_edges_set):\n",
    "        neg_edges = []\n",
    "        nodes = list(G.nodes())\n",
    "\n",
    "        while len(neg_edges) < num_neg_edges:\n",
    "            node1 = random.choice(nodes)\n",
    "            node2 = random.choice(nodes)\n",
    "\n",
    "            if (\n",
    "                node1 != node2\n",
    "                and (node1, node2) not in existing_edges_set\n",
    "                and (node2, node1) not in existing_edges_set\n",
    "            ):\n",
    "                neg_edges.append((node1, node2))\n",
    "\n",
    "        return neg_edges\n",
    "\n",
    "    # Create sets of existing edges for efficient lookup\n",
    "    all_edges_set = set(edges + [(v, u) for u, v in edges])  # Include both directions\n",
    "\n",
    "    # Generate negative examples\n",
    "    num_train_neg = int(len(train_edges) * neg_pos_ratio)\n",
    "    num_test_neg = int(len(test_edges) * neg_pos_ratio)\n",
    "\n",
    "    train_neg_edges = generate_negative_edges(G, num_train_neg, all_edges_set)\n",
    "    test_neg_edges = generate_negative_edges(G, num_test_neg, all_edges_set)\n",
    "\n",
    "    return {\n",
    "        \"train_pos\": train_edges,\n",
    "        \"train_neg\": train_neg_edges,\n",
    "        \"test_pos\": test_edges,\n",
    "        \"test_neg\": test_neg_edges,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prepare_link_prediction_data(G, embeddings, 0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "test_ratio = 0.1\n",
    "data = prepare_link_prediction_data(G, embeddings, test_ratio)\n",
    "ic(data.keys())\n",
    "\n",
    "method = \"hadamard\"\n",
    "method = \"concat\"\n",
    "# Create features for training data\n",
    "train_pos_features = create_edge_features(embeddings, data[\"train_pos\"], method)\n",
    "train_neg_features = create_edge_features(embeddings, data[\"train_neg\"], method)\n",
    "\n",
    "# Create features for test data\n",
    "test_pos_features = create_edge_features(embeddings, data[\"test_pos\"], method)\n",
    "test_neg_features = create_edge_features(embeddings, data[\"test_neg\"], method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pos_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine positive and negative samples\n",
    "X_train = np.vstack([train_pos_features, train_neg_features])\n",
    "y_train = np.hstack(\n",
    "    [np.ones(len(train_pos_features)), np.zeros(len(train_neg_features))]\n",
    ")\n",
    "y_train, X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "# Combine positive and negative samples\n",
    "X_train = np.vstack([train_pos_features, train_neg_features])\n",
    "y_train = np.hstack(\n",
    "    [np.ones(len(train_pos_features)), np.zeros(len(train_neg_features))]\n",
    ")\n",
    "\n",
    "X_test = np.vstack([test_pos_features, test_neg_features])\n",
    "y_test = np.hstack([np.ones(len(test_pos_features)), np.zeros(len(test_neg_features))])\n",
    "\n",
    "# Standardize features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "X_train_final = X_train  # Random Forest doesn't need scaling\n",
    "X_test_final = X_test\n",
    "\n",
    "# Train classifier\n",
    "clf.fit(X_train_final, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = clf.predict(X_test_final)\n",
    "y_pred_proba = clf.predict_proba(X_test_final)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1-Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "results.append(\n",
    "    {\n",
    "        \"method\": method,\n",
    "        \"classifier\": \"random_forest\",\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"auc\": auc,\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Convert results to DataFrame for better visualization\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\n=== Summary Results ===\")\n",
    "print(results_df.round(4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_node_relationship(\n",
    "    node1, node2, embeddings, classifier, scaler, method=\"hadamard\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Predict if two specific nodes have a relationship.\n",
    "\n",
    "    Args:\n",
    "        node1, node2: Node indices\n",
    "        embeddings: Node embeddings matrix\n",
    "        classifier: Trained classifier\n",
    "        scaler: Fitted scaler\n",
    "        method: Feature combination method\n",
    "\n",
    "    Returns:\n",
    "        Prediction probability and binary prediction\n",
    "    \"\"\"\n",
    "    # Create edge feature\n",
    "    edge_feature = create_edge_features(embeddings, [(node1, node2)], method)\n",
    "\n",
    "    # Scale the feature if using logistic regression\n",
    "    if hasattr(classifier, \"predict_proba\"):\n",
    "        try:\n",
    "            edge_feature_scaled = scaler.transform(edge_feature)\n",
    "            prediction_proba = classifier.predict_proba(edge_feature_scaled)[0, 1]\n",
    "            prediction = classifier.predict(edge_feature_scaled)[0]\n",
    "        except:\n",
    "            # For Random Forest, use unscaled features\n",
    "            prediction_proba = classifier.predict_proba(edge_feature)[0, 1]\n",
    "            prediction = classifier.predict(edge_feature)[0]\n",
    "    else:\n",
    "        prediction_proba = classifier.predict_proba(edge_feature)[0, 1]\n",
    "        prediction = classifier.predict(edge_feature)[0]\n",
    "\n",
    "    return prediction_proba, prediction\n",
    "\n",
    "\n",
    "def analyze_node_relationships(G, embeddings, classifier, scaler, method=\"hadamard\"):\n",
    "    \"\"\"\n",
    "    Analyze relationships for all node pairs and compare with actual graph structure.\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes())\n",
    "    predictions = []\n",
    "\n",
    "    print(\"Analyzing node relationships...\")\n",
    "    print(\"Format: (node1, node2) -> probability, prediction, actual\")\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "    # Check some interesting pairs\n",
    "    interesting_pairs = [\n",
    "        (0, 33),  # The two main leaders\n",
    "        (0, 1),  # Direct neighbors\n",
    "        (1, 2),  # Another direct connection\n",
    "        (5, 6),  # Members of same club\n",
    "        (15, 20),  # Members of different clubs\n",
    "        (10, 15),  # Random pair\n",
    "    ]\n",
    "\n",
    "    for node1, node2 in interesting_pairs:\n",
    "        prob, pred = predict_node_relationship(\n",
    "            node1, node2, embeddings, classifier, scaler, method\n",
    "        )\n",
    "        actual = 1 if G.has_edge(node1, node2) else 0\n",
    "\n",
    "        print(\n",
    "            f\"({node1:2d}, {node2:2d}) -> {prob:.4f}, {pred}, {actual} {'✓' if pred == actual else '✗'}\"\n",
    "        )\n",
    "        predictions.append(\n",
    "            {\n",
    "                \"node1\": node1,\n",
    "                \"node2\": node2,\n",
    "                \"probability\": prob,\n",
    "                \"prediction\": pred,\n",
    "                \"actual\": actual,\n",
    "                \"correct\": pred == actual,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "# Test predictions on specific node pairs\n",
    "print(\"\\\\n=== Predicting Specific Node Relationships ===\")\n",
    "predictions = analyze_node_relationships(G, embeddings, trained_clf, scaler)\n",
    "\n",
    "\n",
    "# Create a heatmap of prediction probabilities\n",
    "def create_prediction_heatmap(\n",
    "    G, embeddings, classifier, scaler, method=\"hadamard\", sample_size=None\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a heatmap showing prediction probabilities for node pairs.\n",
    "    \"\"\"\n",
    "    nodes = list(G.nodes())\n",
    "    if sample_size and len(nodes) > sample_size:\n",
    "        nodes = random.sample(nodes, sample_size)\n",
    "\n",
    "    n_nodes = len(nodes)\n",
    "    prob_matrix = np.zeros((n_nodes, n_nodes))\n",
    "\n",
    "    for i, node1 in enumerate(nodes):\n",
    "        for j, node2 in enumerate(nodes):\n",
    "            if i != j:\n",
    "                prob, _ = predict_node_relationship(\n",
    "                    node1, node2, embeddings, classifier, scaler, method\n",
    "                )\n",
    "                prob_matrix[i, j] = prob\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(prob_matrix, cmap=\"RdYlBu_r\", vmin=0, vmax=1)\n",
    "    plt.colorbar(label=\"Relationship Probability\")\n",
    "    plt.title(\"Predicted Relationship Probabilities Between Nodes\")\n",
    "    plt.xlabel(\"Node Index\")\n",
    "    plt.ylabel(\"Node Index\")\n",
    "\n",
    "    # Add tick labels\n",
    "    plt.xticks(range(n_nodes), nodes)\n",
    "    plt.yticks(range(n_nodes), nodes)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return prob_matrix\n",
    "\n",
    "\n",
    "# Create prediction heatmap for first 15 nodes\n",
    "print(\"\\\\n=== Creating Prediction Heatmap ===\")\n",
    "prob_matrix = create_prediction_heatmap(\n",
    "    G, embeddings, trained_clf, scaler, sample_size=15\n",
    ")\n",
    "\n",
    "# Feature importance analysis (for Random Forest)\n",
    "if hasattr(trained_clf, \"feature_importances_\"):\n",
    "    print(\"\\\\n=== Feature Importance Analysis ===\")\n",
    "    feature_importance = trained_clf.feature_importances_\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(range(len(feature_importance)), feature_importance)\n",
    "    plt.title(\"Feature Importance for Link Prediction\")\n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.ylabel(\"Importance\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(f\"Top 5 most important features: {np.argsort(feature_importance)[-5:][::-1]}\")\n",
    "    print(\n",
    "        f\"Their importance scores: {feature_importance[np.argsort(feature_importance)[-5:][::-1]]}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gemini\n",
    "\n",
    "Solution suggested by gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "# Load the Zachary Karate Club graph\n",
    "def load_karate_graph():\n",
    "    \"\"\"\n",
    "    Loads the Zachary Karate Club graph from NetworkX.\n",
    "\n",
    "    Returns:\n",
    "        nx.Graph: The Karate Club graph.\n",
    "    \"\"\"\n",
    "    G = nx.karate_club_graph()\n",
    "    return G\n",
    "\n",
    "\n",
    "# Convert NetworkX graph to adjacency list\n",
    "def graph_to_adj_list(graph):\n",
    "    \"\"\"\n",
    "    Converts a NetworkX graph to an adjacency list representation.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "\n",
    "    Returns:\n",
    "        dict: An adjacency list where keys are nodes and values are lists of neighbors.\n",
    "    \"\"\"\n",
    "    adj_list = defaultdict(list)\n",
    "    for node in graph.nodes():\n",
    "        adj_list[node] = list(graph.neighbors(node))\n",
    "    return adj_list\n",
    "\n",
    "\n",
    "# Generate random walk\n",
    "def generate_random_walk(graph, start_node, walk_length, p, q):\n",
    "    \"\"\"\n",
    "    Generates a random walk starting from a given node.  This implementation\n",
    "    correctly handles the edge weights (p and q) for the walk.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        start_node (int): The starting node for the random walk.\n",
    "        walk_length (int): The length of the random walk.\n",
    "        p (float): The return parameter.\n",
    "        q (float): The in-out parameter.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of nodes representing the random walk.\n",
    "    \"\"\"\n",
    "    walk = [start_node]\n",
    "    for _ in range(walk_length - 1):\n",
    "        current_node = walk[-1]\n",
    "        neighbors = list(graph.neighbors(current_node))\n",
    "        if not neighbors:\n",
    "            break  # Handle disconnected nodes\n",
    "        if len(walk) == 1:\n",
    "            # Start node case: sample directly from neighbors\n",
    "            next_node = random.choice(neighbors)\n",
    "            walk.append(next_node)\n",
    "        else:\n",
    "            # Subsequent step case: consider p and q\n",
    "            previous_node = walk[-2]\n",
    "            probabilities = []\n",
    "            for neighbor in neighbors:\n",
    "                if neighbor == previous_node:\n",
    "                    probabilities.append(1 / p)  # Return probability\n",
    "                elif graph.has_edge(previous_node, neighbor):\n",
    "                    probabilities.append(1)  # Neighboring node\n",
    "                else:\n",
    "                    probabilities.append(1 / q)  # Non-neighboring node\n",
    "            # Normalize probabilities to sum to 1\n",
    "            probabilities = [prob / sum(probabilities) for prob in probabilities]\n",
    "            # Use the probabilities to select the next node.\n",
    "            next_node = random.choices(neighbors, probabilities)[0]\n",
    "            walk.append(next_node)\n",
    "    return walk\n",
    "\n",
    "\n",
    "# Generate multiple random walks\n",
    "def generate_walks(graph, num_walks, walk_length, p, q):\n",
    "    \"\"\"\n",
    "    Generates multiple random walks for each node in the graph.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        num_walks (int): The number of random walks per node.\n",
    "        walk_length (int): The length of each random walk.\n",
    "        p (float): The return parameter.\n",
    "        q (float): The in-out parameter.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of lists, where each inner list is a random walk.\n",
    "    \"\"\"\n",
    "    walks = []\n",
    "    nodes = list(graph.nodes())\n",
    "    for _ in range(num_walks):\n",
    "        random.shuffle(nodes)  # Shuffle for each iteration\n",
    "        for node in nodes:\n",
    "            walk = generate_random_walk(graph, node, walk_length, p, q)\n",
    "            walks.append(walk)\n",
    "    return walks\n",
    "\n",
    "\n",
    "# Skip-gram model with negative sampling\n",
    "class SkipGramModel(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    Skip-gram model for learning node embeddings.  This implementation\n",
    "    uses PyTorch and includes proper initialization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_nodes, embedding_dim):\n",
    "        \"\"\"\n",
    "        Initializes the Skip-gram model.\n",
    "\n",
    "        Args:\n",
    "            num_nodes (int): The number of nodes in the graph.\n",
    "            embedding_dim (int): The dimensionality of the node embeddings.\n",
    "        \"\"\"\n",
    "        super(SkipGramModel, self).__init__()\n",
    "        self.embedding_u = torch.nn.Embedding(\n",
    "            num_nodes, embedding_dim\n",
    "        )  # Target embeddings\n",
    "        self.embedding_v = torch.nn.Embedding(\n",
    "            num_nodes, embedding_dim\n",
    "        )  # Context embeddings\n",
    "\n",
    "        # Initialize embeddings using a small uniform distribution\n",
    "        init_range = 0.5 / embedding_dim\n",
    "        self.embedding_u.weight.data.uniform_(-init_range, init_range)\n",
    "        self.embedding_v.weight.data.uniform_(\n",
    "            -0, 0\n",
    "        )  # init_range, init_range) # Initializing to 0 can sometimes help\n",
    "\n",
    "    def forward(self, center_nodes, context_nodes, negative_samples):\n",
    "        \"\"\"\n",
    "        Computes the forward pass of the Skip-gram model.\n",
    "\n",
    "        Args:\n",
    "            center_nodes (torch.Tensor): Tensor of center node indices.\n",
    "            context_nodes (torch.Tensor): Tensor of context node indices.\n",
    "            negative_samples (torch.Tensor): Tensor of negative sample indices.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The loss value.\n",
    "        \"\"\"\n",
    "        # Get embeddings for center nodes\n",
    "        u_embeddings = self.embedding_u(center_nodes)  # (batch_size, embedding_dim)\n",
    "\n",
    "        # Get embeddings for context nodes\n",
    "        v_embeddings = self.embedding_v(context_nodes)  # (batch_size, embedding_dim)\n",
    "\n",
    "        # Compute the positive log-likelihood\n",
    "        positive_scores = torch.sum(u_embeddings * v_embeddings, dim=1)  # (batch_size,)\n",
    "        positive_loss = -torch.mean(torch.log(torch.sigmoid(positive_scores)))\n",
    "\n",
    "        # Get embeddings for negative samples\n",
    "        negative_embeddings = self.embedding_v(\n",
    "            negative_samples\n",
    "        )  # (batch_size, num_negative_samples, embedding_dim)\n",
    "\n",
    "        # Compute the negative log-likelihood\n",
    "        negative_scores = torch.bmm(\n",
    "            negative_embeddings, u_embeddings.unsqueeze(2)\n",
    "        ).squeeze(2)  # (batch_size, num_negative_samples)\n",
    "        negative_loss = -torch.mean(torch.log(torch.sigmoid(-negative_scores)))\n",
    "\n",
    "        # Combine positive and negative losses\n",
    "        loss = positive_loss + negative_loss\n",
    "        return loss\n",
    "\n",
    "\n",
    "def generate_negative_samples(walks, num_nodes, window_size, num_negative_samples):\n",
    "    \"\"\"\n",
    "    Generates negative samples for each center node and context node pair.\n",
    "    This version pre-computes the unigram distribution and samples directly\n",
    "    according to that distribution.\n",
    "\n",
    "    Args:\n",
    "        walks (list): A list of random walks.\n",
    "        num_nodes (int): The total number of nodes in the graph.\n",
    "        window_size (int): The context window size.\n",
    "        num_negative_samples (int): The number of negative samples per (center, context) pair.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains:\n",
    "            (center_node, context_node, list of negative samples)\n",
    "    \"\"\"\n",
    "    negative_samples = []\n",
    "    node_counts = [0] * num_nodes  # Initialize node counts\n",
    "    for walk in walks:\n",
    "        for node in walk:\n",
    "            node_counts[node] += 1\n",
    "\n",
    "    # Compute the unigram distribution raised to the power of 0.75\n",
    "    total_count = sum(node_counts)\n",
    "    unigram_dist = [(count / total_count) ** 0.75 for count in node_counts]\n",
    "    # Normalize the distribution\n",
    "    unigram_dist = [p / sum(unigram_dist) for p in unigram_dist]\n",
    "\n",
    "    for walk in walks:\n",
    "        for i, center_node in enumerate(walk):\n",
    "            for j in range(\n",
    "                max(0, i - window_size), min(len(walk), i + window_size + 1)\n",
    "            ):\n",
    "                if i != j:\n",
    "                    context_node = walk[j]\n",
    "                    # Generate negative samples, ensuring they are different from the context node\n",
    "                    neg_samples = random.choices(\n",
    "                        range(num_nodes), weights=unigram_dist, k=num_negative_samples\n",
    "                    )\n",
    "                    negative_samples.append((center_node, context_node, neg_samples))\n",
    "    return negative_samples\n",
    "\n",
    "\n",
    "def train_node2vec(\n",
    "    graph,\n",
    "    walks,\n",
    "    embedding_dim,\n",
    "    window_size,\n",
    "    num_negative_samples,\n",
    "    batch_size,\n",
    "    epochs,\n",
    "    learning_rate,\n",
    "    p,\n",
    "    q,\n",
    "    num_workers=0,\n",
    "):\n",
    "    \"\"\"\n",
    "    Trains the Node2vec model. This function incorporates the negative sampling\n",
    "    and training loop, and uses the graph, walks, and p, q parameters.\n",
    "\n",
    "    Args:\n",
    "        graph (nx.Graph): The input graph.\n",
    "        walks (list): A list of random walks.\n",
    "        embedding_dim (int): The dimensionality of the node embeddings.\n",
    "        window_size (int): The context window size.\n",
    "        num_negative_samples (int): The number of negative samples per (center, context) pair.\n",
    "        batch_size (int): The batch size for training.\n",
    "        epochs (int): The number of training epochs.\n",
    "        learning_rate (float): The learning rate.\n",
    "        p (float): The return parameter.\n",
    "        q (float): The in-out parameter.\n",
    "        num_workers (int): Number of workers for data loading (not used in this basic implementation).\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: The trained Skip-gram model.\n",
    "    \"\"\"\n",
    "    # Determine the device to use\n",
    "    device = (\n",
    "        torch.device(\"mps\")\n",
    "        if torch.backends.mps.is_available()\n",
    "        else torch.device(\"cpu\")\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    num_nodes = graph.number_of_nodes()\n",
    "    model = SkipGramModel(num_nodes, embedding_dim).to(device)  # Move model to device\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    # Use Binary Cross Entropy Loss (though we calculate it manually for more control)\n",
    "    # criterion = torch.nn.BCEWithLogitsLoss() # Not used, but kept here for reference\n",
    "\n",
    "    negative_samples = generate_negative_samples(\n",
    "        walks, num_nodes, window_size, num_negative_samples\n",
    "    )\n",
    "\n",
    "    # Prepare data for PyTorch (create tensors) outside the loop\n",
    "    center_nodes_all = []\n",
    "    context_nodes_all = []\n",
    "    negative_samples_all = []\n",
    "\n",
    "    for center_node, context_node, neg_samples in negative_samples:\n",
    "        center_nodes_all.append(center_node)\n",
    "        context_nodes_all.append(context_node)\n",
    "        negative_samples_all.append(neg_samples)\n",
    "\n",
    "    center_nodes_tensor = torch.tensor(center_nodes_all, dtype=torch.long).to(\n",
    "        device\n",
    "    )  # Move tensors to device\n",
    "    context_nodes_tensor = torch.tensor(context_nodes_all, dtype=torch.long).to(device)\n",
    "    negative_samples_tensor = torch.tensor(negative_samples_all, dtype=torch.long).to(\n",
    "        device\n",
    "    )\n",
    "\n",
    "    num_samples = len(negative_samples)\n",
    "    for epoch in range(epochs):\n",
    "        # Shuffle the training data at the beginning of each epoch\n",
    "        permutation = torch.randperm(num_samples).to(\n",
    "            device\n",
    "        )  # Move permutation to device\n",
    "        center_nodes_tensor = center_nodes_tensor[permutation]\n",
    "        context_nodes_tensor = context_nodes_tensor[permutation]\n",
    "        negative_samples_tensor = negative_samples_tensor[permutation]\n",
    "        total_loss = 0.0\n",
    "        for i in tqdm(\n",
    "            range(0, num_samples, batch_size), desc=f\"Epoch {epoch + 1}/{epochs}\"\n",
    "        ):\n",
    "            optimizer.zero_grad()\n",
    "            # Get batch\n",
    "            indices = torch.arange(i, min(i + batch_size, num_samples)).to(\n",
    "                device\n",
    "            )  # Move indices to device\n",
    "            batch_center_nodes = center_nodes_tensor[indices]\n",
    "            batch_context_nodes = context_nodes_tensor[indices]\n",
    "            batch_negative_samples = negative_samples_tensor[indices]\n",
    "\n",
    "            # Forward pass\n",
    "            loss = model(\n",
    "                batch_center_nodes, batch_context_nodes, batch_negative_samples\n",
    "            )\n",
    "            total_loss += loss.item()\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(\n",
    "            f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / (num_samples // batch_size)}\"\n",
    "        )\n",
    "    return model\n",
    "\n",
    "\n",
    "def get_node_embeddings(model):\n",
    "    \"\"\"\n",
    "    Gets the node embeddings from the trained model.\n",
    "\n",
    "    Args:\n",
    "        model (torch.nn.Module): The trained Skip-gram model.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: A tensor containing the node embeddings.\n",
    "    \"\"\"\n",
    "    return model.embedding_u.weight.data.cpu().numpy()\n",
    "\n",
    "\n",
    "def visualize_embeddings(embeddings, graph, title=\"Node2vec Embeddings\"):\n",
    "    \"\"\"\n",
    "    Visualizes the node embeddings using a 2D scatter plot.  This version\n",
    "    uses the actual node labels from the Karate Club graph.\n",
    "\n",
    "    Args:\n",
    "        embeddings (numpy.ndarray): The node embeddings.\n",
    "        graph (nx.Graph): The input graph (for getting node labels).\n",
    "        title (str): The title of the plot.\n",
    "    \"\"\"\n",
    "    # Use a simple PCA for dimensionality reduction to 2D\n",
    "    from sklearn.decomposition import PCA\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Get the club labels for coloring\n",
    "    club_labels = [graph.nodes[node][\"club\"] for node in graph.nodes()]\n",
    "    colors = [\"red\" if label == \"Mr. Hi\" else \"blue\" for label in club_labels]\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1], c=colors)\n",
    "\n",
    "    # Add node labels to the plot\n",
    "    for i, node in enumerate(graph.nodes()):\n",
    "        plt.annotate(\n",
    "            str(node),\n",
    "            xy=(reduced_embeddings[i, 0], reduced_embeddings[i, 1]),\n",
    "            xytext=(5, 2),\n",
    "            textcoords=\"offset points\",\n",
    "            fontsize=10,\n",
    "        )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"Dimension 1\")\n",
    "    plt.ylabel(\"Dimension 2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Hyperparameters\n",
    "    embedding_dim = 128\n",
    "    walk_length = 20\n",
    "    num_walks = 10\n",
    "    window_size = 5\n",
    "    num_negative_samples = 5\n",
    "    batch_size = 32\n",
    "    epochs = 50\n",
    "    learning_rate = 0.05  # Increased learning rate\n",
    "    p = 1.0\n",
    "    q = 1.0\n",
    "\n",
    "    # Load graph\n",
    "    karate_graph = load_karate_graph()\n",
    "\n",
    "    # Generate random walks\n",
    "    walks = generate_walks(karate_graph, num_walks, walk_length, p, q)\n",
    "\n",
    "    # Train Node2vec model\n",
    "    model = train_node2vec(\n",
    "        karate_graph,\n",
    "        walks,\n",
    "        embedding_dim,\n",
    "        window_size,\n",
    "        num_negative_samples,\n",
    "        batch_size,\n",
    "        epochs,\n",
    "        learning_rate,\n",
    "        p,\n",
    "        q,\n",
    "    )\n",
    "\n",
    "    # Get node embeddings\n",
    "    node_embeddings = get_node_embeddings(model)\n",
    "\n",
    "    # Visualize embeddings\n",
    "    visualize_embeddings(\n",
    "        node_embeddings, karate_graph, title=\"Node2vec Embeddings (Karate Club)\"\n",
    "    )\n",
    "\n",
    "    # Print the first 5 embeddings\n",
    "    print(\"First 5 Node Embeddings:\")\n",
    "    print(node_embeddings[:5])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-gnn-py3.12 (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
