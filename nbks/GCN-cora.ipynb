{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e35125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import random\n",
    "from icecream import ic\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74838096",
   "metadata": {},
   "source": [
    "# Cora dataset\n",
    "\n",
    "https://graphsandnetworks.com/the-cora-dataset/\n",
    "\n",
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
    "\n",
    "Load cora:\n",
    "\n",
    "- D_inv is $D_{v,v}=Deg(v) = |N(v)|$, Deg is the nb of neighbours that each node has.\n",
    "- adj_bar will be reused in $H^{(k+1)}=D^{-1}AH^{(k)}$. We will multiply by H at each layer.\n",
    "- x: the features (words within doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset\n",
    "\n",
    "# Download and extract if not present\n",
    "if not os.path.exists(\"cora\"):\n",
    "    print(\"Downloading Cora dataset...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\", \"cora.tgz\"\n",
    "    )\n",
    "    with tarfile.open(\"cora.tgz\", \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "# Load node features and labels\n",
    "content_path = \"cora/cora.content\"\n",
    "content = np.genfromtxt(content_path, dtype=str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = content[:, 0].astype(int)\n",
    "features = content[:, 1:-1].astype(np.float32)\n",
    "class_strs = content[:, -1]\n",
    "\n",
    "N = len(node_ids)\n",
    "node_map = {node_id: i for i, node_id in enumerate(node_ids)}\n",
    "\n",
    "# Features matrix\n",
    "X = features  # Already in shape (N, 1433)\n",
    "\n",
    "# Labels: 7 classes\n",
    "class_map = {\n",
    "    \"Case_Based\": 0,\n",
    "    \"Genetic_Algorithms\": 1,\n",
    "    \"Neural_Networks\": 2,\n",
    "    \"Probabilistic_Methods\": 3,\n",
    "    \"Reinforcement_Learning\": 4,\n",
    "    \"Rule_Learning\": 5,\n",
    "    \"Theory\": 6,\n",
    "}\n",
    "labels = np.array([class_map[c] for c in class_strs])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226977b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b71aa6cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edges (directed citations)\n",
    "cites_path = \"cora/cora.cites\"\n",
    "cites = np.genfromtxt(cites_path, dtype=int)\n",
    "cites\n",
    "\n",
    "edges = []\n",
    "for src, dst in cites:\n",
    "    if src in node_map and dst in node_map:  # Ensure both exist\n",
    "        src_idx = node_map[src]\n",
    "        dst_idx = node_map[dst]\n",
    "        edges.append([src_idx, dst_idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d21a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix (undirected, no self-loops)\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "for u, v in edges:\n",
    "    A[u, v] = 1.0Â \n",
    "    # A[v, u] = 1.0  # Make undirected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c10b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see page 50 of 03-GNN1.pdf\n",
    "#\n",
    "degrees = np.sum(A, axis=1)\n",
    "degrees[degrees == 0] = 1.0\n",
    "degrees\n",
    "# Avoid division by zero\n",
    "D_inv = np.diag(1.0 / degrees)\n",
    "D_inv\n",
    "\n",
    "\n",
    "# adj_bar will be used in $$D^{-1}AH^{(k)}$$\n",
    "\n",
    "adj_bar = D_inv @ A\n",
    "adj_bar = torch.FloatTensor(adj_bar)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df97d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(X)\n",
    "pos_edges = np.array(edges)  # Directed for positive samples\n",
    "pos_edges\n",
    "\n",
    "# x, adj_bar, pos_edges, labels, N = load_cora()\n",
    "ic(x.shape, adj_bar.shape, pos_edges.shape, labels.shape, degrees.shape, N, set(labels))\n",
    "features_nb = x.shape[1]\n",
    "ic(features_nb);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3432ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network visualization of a subset of the graph\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Class distribution\n",
    "class_names = [\n",
    "    \"Case_Based\",\n",
    "    \"Genetic_Algorithms\",\n",
    "    \"Neural_Networks\",\n",
    "    \"Probabilistic_Methods\",\n",
    "    \"Reinforcement_Learning\",\n",
    "    \"Rule_Learning\",\n",
    "    \"Theory\",\n",
    "]\n",
    "\n",
    "# Create a NetworkX graph from a subset of nodes (for visualization purposes)\n",
    "# Using only nodes with high degree to keep visualization manageable\n",
    "# threshold  of degree where node degree is in the top 10%\n",
    "high_degree_threshold = np.percentile(degrees, 50)  # Top 10% by degree\n",
    "\n",
    "high_degree_nodes = np.where(degrees >= high_degree_threshold)[0]\n",
    "\n",
    "print(\n",
    "    f\"Visualizing subgraph with {len(high_degree_nodes)} high-degree nodes (degree >= {high_degree_threshold:.0f})\"\n",
    ")\n",
    "\n",
    "# Create subgraph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(high_degree_nodes)\n",
    "\n",
    "# Add edges between high-degree nodes\n",
    "for u, v in pos_edges:\n",
    "    if u in high_degree_nodes and v in high_degree_nodes:\n",
    "        G.add_edge(u, v)\n",
    "\n",
    "print(f\"Subgraph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Create node colors based on class labels\n",
    "node_colors = [labels[node] for node in G.nodes()]\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    node_color=[colors[c] for c in node_colors],\n",
    "    node_size=50,\n",
    "    edge_color=\"gray\",\n",
    "    alpha=0.8,\n",
    "    with_labels=False,\n",
    ")\n",
    "\n",
    "# Create legend\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=colors[i], label=class_names[i]) for i in range(len(class_names))\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.title(\n",
    "    \"Cora Citation Network (High-degree nodes subgraph)\\nColored by Publication Class\"\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e637cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alias_edge(G, src, curr, p, q):\n",
    "    \"\"\"Compute transition probabilities for neighbors of 'dst' based on Node2Vec bias.\n",
    "    dst: current node\n",
    "    src: previous node\n",
    "    p: return back to previous node. Lower p means backtracking is more likely.\n",
    "    q: in-out param: explore new nodes (ratio of bfs[breadth-first search] to dfs[depth-first search]).\n",
    "        lower q means more exploration of distant nodes\n",
    "    \"\"\"\n",
    "    unnormalized_probs = []\n",
    "    for neighbor in G[curr]:\n",
    "        if neighbor == src:\n",
    "            weight = 1.0 / p\n",
    "        elif G.has_edge(\n",
    "            neighbor, src\n",
    "        ):  # if neighbor is a neighbor of src (above ex: s1 is neighbor of t)\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            weight = 1.0 / q\n",
    "        unnormalized_probs.append(weight)\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs = [w / norm_const for w in unnormalized_probs]\n",
    "    return list(G[curr]), normalized_probs\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start, walk_length, p, q):\n",
    "    \"\"\"Generate a random walk starting from the given node.\"\"\"\n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        neighbors = list(G.neighbors(cur))\n",
    "        if len(neighbors) == 0:\n",
    "            break\n",
    "        if len(walk) == 1:\n",
    "            next_node = random.choice(neighbors)\n",
    "        else:\n",
    "            prev = walk[-2]\n",
    "            candidates, probs = get_alias_edge(G, src=prev, curr=cur, p=p, q=q)\n",
    "            next_node = random.choices(candidates, weights=probs, k=1)[0]\n",
    "        walk.append(next_node)\n",
    "    return walk\n",
    "\n",
    "\n",
    "def gen_walk_pairs(walk, window_size):\n",
    "    walk_pairs = []\n",
    "    for i, target in enumerate(walk):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(walk), i + window_size + 1)\n",
    "        ic(i, target, start, end)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                ic(\"pair\", target, j, walk[j])\n",
    "                if walk[j] != target:  # Avoid pairs of the same node\n",
    "                    walk_pairs.append((target, walk[j]))\n",
    "\n",
    "    return walk_pairs\n",
    "\n",
    "\n",
    "def generate_random_walks(adj: np.numarray, walk_length=10, num_walks=5):\n",
    "    G = nx.from_numpy_array(adj)\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        nodes = list(G.nodes())\n",
    "        random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walk = node2vec_walk(G, node, walk_length, p=1, q=1)\n",
    "            walks.append(walk)\n",
    "    return walks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "q = 0.2\n",
    "walk_length = 10\n",
    "num_walks = 2\n",
    "walks = generate_random_walks(\n",
    "    adj=adj_bar.numpy(), walk_length=walk_length, num_walks=num_walks\n",
    ")\n",
    "ic(len(walks), walks[:2])\n",
    "\n",
    "ic.disable()\n",
    "pos_pairs = []\n",
    "for walk in walks:\n",
    "    pairs = gen_walk_pairs(walk, window_size=2)\n",
    "    pos_pairs.extend(pairs)\n",
    "\n",
    "num_positive_samples = len(pos_pairs)\n",
    "ic(num_positive_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ef4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative samples (random non-walk pairs)\n",
    "ic.enable()\n",
    "pos_pair_set = set(tuple(p) for p in pos_pairs)\n",
    "ic(len(pos_pair_set))\n",
    "\n",
    "neg_pairs = []\n",
    "num_negative_samples = 0\n",
    "while num_negative_samples < num_positive_samples:\n",
    "    u, v = np.random.randint(0, N, 2)\n",
    "    if u != v and (u, v) not in pos_pair_set:\n",
    "        neg_pairs.append([u, v])\n",
    "        num_negative_samples += 1\n",
    "\n",
    "ic(len(neg_pairs), neg_pairs[:5]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd7995",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c90238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN Layer (with separate W for neighbors and B for self)\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.FloatTensor(in_dim, out_dim))\n",
    "        self.B = nn.Parameter(torch.FloatTensor(in_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.xavier_uniform_(self.B)\n",
    "\n",
    "    def forward(self, h, adj_bar):\n",
    "        # Neighbor aggregation: adj_bar @ h @ W (where adj_bar = D^{-1} A).\n",
    "        # adj_bar stands for adjusted adjacency matrix.\n",
    "        # torch.mm is optimized for 2d matrix operations: faster, less memory,..\n",
    "        neigh_h = torch.mm(adj_bar, h)\n",
    "        neigh_h = torch.mm(neigh_h, self.W)\n",
    "        # Self transformation: h @ B\n",
    "        self_h = torch.mm(h, self.B)\n",
    "        # Add them\n",
    "        return neigh_h + self_h\n",
    "\n",
    "\n",
    "# GNN Encoder\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.layer1 = GNNLayer(\n",
    "            input_dim, hidden_dim\n",
    "        )  # Map features size to hidden size\n",
    "        self.layer2 = GNNLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GNNLayer(hidden_dim, embedding_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj_bar):\n",
    "        h = self.layer1(x, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer2(h, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer3(h, adj_bar)  # No activation on last layer for embeddings\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b950467",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNEncoder(input_dim=features_nb, hidden_dim=32, embedding_dim=16).to(device)\n",
    "z = model(x.to(device), adj_bar.to(device))\n",
    "ic(\n",
    "    z.shape\n",
    ")  # Should be (N, 16), N being number of nodes. As expected, we have a 16-dim embedding for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder: compute logit (dot product) as similarity measure. Like in node2vec or others previously seen.\n",
    "def decode(z, edges):\n",
    "    return torch.sum(z[edges[:, 0]] * z[edges[:, 1]], dim=1)\n",
    "\n",
    "\n",
    "ic(\"There are \", pos_edges.shape[0], \" edges in the graph. -> positive samples.\")\n",
    "ic(pos_edges.shape)\n",
    "pos_edges_tensor = torch.LongTensor(pos_edges).to(device)\n",
    "ic(decode(z, pos_edges_tensor).shape);  # Should be (num_pos,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# x, adj_bar, pos_edges, labels, N = load_cora()\n",
    "# data has been loaded above\n",
    "model = GNNEncoder(input_dim=features_nb, hidden_dim=32, embedding_dim=16).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Use randow walk\n",
    "num_positive_pairs = len(pos_pairs)\n",
    "num_negative_pairs = len(neg_pairs)\n",
    "ic(num_positive_pairs, num_negative_pairs)\n",
    "neg_pairs_tensor = torch.LongTensor(neg_pairs).to(device)\n",
    "pos_pairs_tensor = torch.LongTensor(pos_pairs).to(device)\n",
    "\n",
    "x = x.to(device)\n",
    "adj_bar = adj_bar.to(device)  # (where adj_bar = D^{-1} A)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model(x, adj_bar)\n",
    "\n",
    "    # Positive logits\n",
    "    # for positive edges, decode similarity should be high (1)\n",
    "    pos_logits = decode(z, pos_pairs_tensor)\n",
    "\n",
    "    # for negative edges, decode similarity should be low (0)\n",
    "    neg_logits = decode(z, neg_pairs_tensor)\n",
    "\n",
    "    # Labels: 1 for pos, 0 for neg\n",
    "    pos_labels = torch.ones(num_positive_pairs).to(device=device)\n",
    "    neg_labels = torch.zeros(num_negative_pairs).to(device=device)\n",
    "\n",
    "    # Loss\n",
    "    loss = binary_cross_entropy_with_logits(\n",
    "        pos_logits, pos_labels\n",
    "    ) + binary_cross_entropy_with_logits(neg_logits, neg_labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# return model, x, adj_bar, labels\n",
    "ic(model, x, adj_bar, labels);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generate embeddings\n",
    "def inference(model, x, adj_bar):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(x, adj_bar)\n",
    "    return z.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def visualize_tsne_embeddings(z, labels):\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "    z_2d = tsne.fit_transform(z)\n",
    "\n",
    "    colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, lbl in enumerate(unique_labels):\n",
    "        mask = labels == lbl\n",
    "        plt.scatter(\n",
    "            z_2d[mask, 0],\n",
    "            z_2d[mask, 1],\n",
    "            c=colors[i % len(colors)],\n",
    "            label=f\"Class {lbl}\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.title(\"2D Projection of Node Embeddings on Cora (t-SNE, colored by class)\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "z = inference(model, x, adj_bar)\n",
    "visualize_tsne_embeddings(z, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e698b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def visualize_tsne_embeddings_3d(z, labels):\n",
    "    tsne = TSNE(n_components=3, random_state=42, perplexity=30, max_iter=1000)\n",
    "    z_3d = tsne.fit_transform(z)\n",
    "\n",
    "    colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    for i, lbl in enumerate(unique_labels):\n",
    "        mask = labels == lbl\n",
    "        ax.scatter(\n",
    "            z_3d[mask, 0],\n",
    "            z_3d[mask, 1],\n",
    "            z_3d[mask, 2],\n",
    "            c=colors[i % len(colors)],\n",
    "            label=f\"Class {lbl}\",\n",
    "            alpha=0.7,\n",
    "            s=20,  # point size\n",
    "        )\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(\"3D Projection of Node Embeddings on Cora (t-SNE, colored by class)\")\n",
    "    ax.set_xlabel(\"t-SNE Component 1\")\n",
    "    ax.set_ylabel(\"t-SNE Component 2\")\n",
    "    ax.set_zlabel(\"t-SNE Component 3\")\n",
    "\n",
    "    # Enable rotation\n",
    "    ax.view_init(elev=20, azim=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate 3D t-SNE visualization\n",
    "visualize_tsne_embeddings_3d(z, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74082c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "\n",
    "def visualize_embeddings_tensorboard(z, labels, class_names, log_dir=\"runs/embeddings\"):\n",
    "    \"\"\"\n",
    "    Visualize embeddings in TensorBoard with interactive 3D projection\n",
    "    \"\"\"\n",
    "    # Create log directory\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(z, np.ndarray):\n",
    "        embeddings = torch.FloatTensor(z)\n",
    "    else:\n",
    "        embeddings = z\n",
    "\n",
    "    # Create metadata (class labels)\n",
    "    metadata = [class_names[label] for label in labels]\n",
    "\n",
    "    # Add embeddings to TensorBoard\n",
    "    writer.add_embedding(embeddings, metadata=metadata, tag=\"Node_Embeddings\")\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Embeddings saved to TensorBoard. Run the following command to view:\")\n",
    "    print(f\"tensorboard --logdir=nbks/{log_dir}\")\n",
    "    print(\"Then open your browser to http://localhost:6006\")\n",
    "    print(\"Navigate to the 'Projector' tab to see the interactive 3D visualization\")\n",
    "\n",
    "\n",
    "# Generate TensorBoard visualization\n",
    "class_names = [\n",
    "    \"Case_Based\",\n",
    "    \"Genetic_Algorithms\",\n",
    "    \"Neural_Networks\",\n",
    "    \"Probabilistic_Methods\",\n",
    "    \"Reinforcement_Learning\",\n",
    "    \"Rule_Learning\",\n",
    "    \"Theory\",\n",
    "]\n",
    "\n",
    "visualize_embeddings_tensorboard(z, labels, class_names)\n",
    "\n",
    "# uv run tensorboard --logdir nbks/runs/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e366e2f",
   "metadata": {},
   "source": [
    "# use classification instead of similarity as target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea005bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim, num_classes):\n",
    "        super(GNNClassifier, self).__init__()\n",
    "        self.layer1 = GNNLayer(input_dim, hidden_dim)\n",
    "        self.layer2 = GNNLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GNNLayer(hidden_dim, embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj_bar):\n",
    "        h = self.layer1(x, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer2(h, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer3(h, adj_bar)  # Embeddings\n",
    "        logits = self.classifier(h)  # Classification logits\n",
    "        return h, logits  # Return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(set(labels.tolist()))\n",
    "y = torch.LongTensor(labels).to(device)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNClassifier(\n",
    "    input_dim=features_nb, hidden_dim=32, embedding_dim=16, num_classes=nb_classes\n",
    ").to(device)\n",
    "h, train_logits = model(x.to(device), adj_bar.to(device))\n",
    "ic(h.shape, train_logits.shape)  # h: (N, 16), logits: (N, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ed99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNClassifier(\n",
    "    input_dim=1433, hidden_dim=32, embedding_dim=16, num_classes=7\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Split into train/val/test (60%/20%/20%)\n",
    "idx = np.arange(N)\n",
    "train_idx, test_idx = train_test_split(idx, test_size=0.4, random_state=42)\n",
    "val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)\n",
    "train_idx = torch.LongTensor(train_idx).to(device)\n",
    "val_idx = torch.LongTensor(val_idx).to(device)\n",
    "test_idx = torch.LongTensor(test_idx).to(device)\n",
    "\n",
    "# Subset train data\n",
    "train_x = x[train_idx]\n",
    "train_adj = adj_bar[train_idx][:, train_idx]  # Subgraph adjacency\n",
    "train_y = y[train_idx]\n",
    "train_labels = labels[train_idx.cpu().numpy()]\n",
    "\n",
    "val_x = x[val_idx]\n",
    "val_adj = adj_bar[val_idx][:, val_idx]  # Subgraph adjacency\n",
    "val_y = y[val_idx]\n",
    "val_labels = labels[val_idx.cpu().numpy()]\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_embeddings, train_logits = model(train_x, train_adj)  # Train on subgraph\n",
    "\n",
    "    # Compute loss on training set\n",
    "    train_loss = cross_entropy(train_logits, train_y)\n",
    "\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation on full graph\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_embeddings, val_logits = model(\n",
    "            val_x, val_adj\n",
    "        )  # Use full graph for embeddings\n",
    "        val_loss = cross_entropy(val_logits, val_y)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "# Load best model and generate full embeddings\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    full_embeddings, _ = model(x, adj_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne_embeddings(train_embeddings.detach().cpu().numpy(), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d102899",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne_embeddings(val_embeddings.cpu().numpy(), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f88b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne_embeddings(full_embeddings.cpu().numpy(), labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0cbab",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-gnn-py3.12 (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
