{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e35125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import random\n",
    "from icecream import ic\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74838096",
   "metadata": {},
   "source": [
    "# Cora dataset\n",
    "\n",
    "https://graphsandnetworks.com/the-cora-dataset/\n",
    "\n",
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
    "\n",
    "Load cora:\n",
    "\n",
    "- D_inv is $D_{v,v}=Deg(v) = |N(v)|$, Deg is the nb of neighbours that each node has.\n",
    "- adj_bar will be reused in $H^{(k+1)}=D^{-1}AH^{(k)}$. We will multiply by H at each layer.\n",
    "- x: the features (words within doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset\n",
    "\n",
    "# Download and extract if not present\n",
    "if not os.path.exists(\"cora\"):\n",
    "    print(\"Downloading Cora dataset...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\", \"cora.tgz\"\n",
    "    )\n",
    "    with tarfile.open(\"cora.tgz\", \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "# Load node features and labels\n",
    "content_path = \"cora/cora.content\"\n",
    "content = np.genfromtxt(content_path, dtype=str)\n",
    "node_ids = content[:, 0].astype(int)\n",
    "features = content[:, 1:-1].astype(np.float32)\n",
    "class_strs = content[:, -1]\n",
    "\n",
    "N = len(node_ids)\n",
    "node_map = {node_id: i for i, node_id in enumerate(node_ids)}\n",
    "\n",
    "# Features matrix\n",
    "X = features  # Already in shape (N, 1433)\n",
    "\n",
    "# Labels: 7 classes\n",
    "class_map = {\n",
    "    \"Case_Based\": 0,\n",
    "    \"Genetic_Algorithms\": 1,\n",
    "    \"Neural_Networks\": 2,\n",
    "    \"Probabilistic_Methods\": 3,\n",
    "    \"Reinforcement_Learning\": 4,\n",
    "    \"Rule_Learning\": 5,\n",
    "    \"Theory\": 6,\n",
    "}\n",
    "labels = np.array([class_map[c] for c in class_strs])\n",
    "\n",
    "# Load edges (directed citations)\n",
    "cites_path = \"cora/cora.cites\"\n",
    "cites = np.genfromtxt(cites_path, dtype=int)\n",
    "edges = []\n",
    "for src, dst in cites:\n",
    "    if src in node_map and dst in node_map:  # Ensure both exist\n",
    "        src_idx = node_map[src]\n",
    "        dst_idx = node_map[dst]\n",
    "        edges.append([src_idx, dst_idx])\n",
    "\n",
    "# Adjacency matrix (undirected, no self-loops)\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "for u, v in edges:\n",
    "    A[u, v] = 1.0\n",
    "    A[v, u] = 1.0  # Make undirected\n",
    "\n",
    "# see page 50 of 03-GNN1.pdf\n",
    "#\n",
    "degrees = np.sum(A, axis=1)\n",
    "degrees[degrees == 0] = 1.0  # Avoid division by zero\n",
    "D_inv = np.diag(1.0 / degrees)\n",
    "adj_bar = D_inv @ A\n",
    "adj_bar = torch.FloatTensor(adj_bar)\n",
    "# adj_bar will be used in $$D^{-1}AH^{(k)}$$\n",
    "\n",
    "x = torch.FloatTensor(X)\n",
    "pos_edges = np.array(edges)  # Directed for positive samples\n",
    "\n",
    "# return x, adj_bar, pos_edges, labels, N\n",
    "\n",
    "\n",
    "# x, adj_bar, pos_edges, labels, N = load_cora()\n",
    "ic(x.shape, adj_bar.shape, pos_edges.shape, labels.shape, degrees.shape, N, set(labels))\n",
    "features_nb = x.shape[1]\n",
    "ic(features_nb);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3432ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network visualization of a subset of the graph\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Class distribution\n",
    "class_names = [\n",
    "    \"Case_Based\",\n",
    "    \"Genetic_Algorithms\",\n",
    "    \"Neural_Networks\",\n",
    "    \"Probabilistic_Methods\",\n",
    "    \"Reinforcement_Learning\",\n",
    "    \"Rule_Learning\",\n",
    "    \"Theory\",\n",
    "]\n",
    "\n",
    "# Create a NetworkX graph from a subset of nodes (for visualization purposes)\n",
    "# Using only nodes with high degree to keep visualization manageable\n",
    "# threshold  of degree where node degree is in the top 10%\n",
    "high_degree_threshold = np.percentile(degrees, 50)  # Top 10% by degree\n",
    "\n",
    "high_degree_nodes = np.where(degrees >= high_degree_threshold)[0]\n",
    "\n",
    "print(\n",
    "    f\"Visualizing subgraph with {len(high_degree_nodes)} high-degree nodes (degree >= {high_degree_threshold:.0f})\"\n",
    ")\n",
    "\n",
    "# Create subgraph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(high_degree_nodes)\n",
    "\n",
    "# Add edges between high-degree nodes\n",
    "for u, v in pos_edges:\n",
    "    if u in high_degree_nodes and v in high_degree_nodes:\n",
    "        G.add_edge(u, v)\n",
    "\n",
    "print(f\"Subgraph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Create node colors based on class labels\n",
    "node_colors = [labels[node] for node in G.nodes()]\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    node_color=[colors[c] for c in node_colors],\n",
    "    node_size=50,\n",
    "    edge_color=\"gray\",\n",
    "    alpha=0.8,\n",
    "    with_labels=False,\n",
    ")\n",
    "\n",
    "# Create legend\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=colors[i], label=class_names[i]) for i in range(len(class_names))\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.title(\n",
    "    \"Cora Citation Network (High-degree nodes subgraph)\\nColored by Publication Class\"\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c90238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN Layer (with separate W for neighbors and B for self)\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.FloatTensor(in_dim, out_dim))\n",
    "        self.B = nn.Parameter(torch.FloatTensor(in_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.xavier_uniform_(self.B)\n",
    "\n",
    "    def forward(self, h, adj_bar):\n",
    "        # Neighbor aggregation: adj_bar @ h @ W (where adj_bar = D^{-1} A). adj_bar is adjusted adjacency matrix.\n",
    "        neigh_h = torch.mm(adj_bar, h)\n",
    "        neigh_h = torch.mm(neigh_h, self.W)\n",
    "        # Self transformation: h @ B\n",
    "        self_h = torch.mm(h, self.B)\n",
    "        # Add them\n",
    "        return neigh_h + self_h\n",
    "\n",
    "\n",
    "# GNN Encoder\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.layer1 = GNNLayer(\n",
    "            input_dim, hidden_dim\n",
    "        )  # Map features size to hidden size\n",
    "        self.layer2 = GNNLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GNNLayer(hidden_dim, embedding_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj_bar):\n",
    "        h = self.layer1(x, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer2(h, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer3(h, adj_bar)  # No activation on last layer for embeddings\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2398008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative sampling (sample non-edges)\n",
    "def sample_negative_edges(pos_edges, N, num_samples):\n",
    "    neg_edges = []\n",
    "    pos_set = set(tuple(e) for e in pos_edges)  # Directed check\n",
    "    while len(neg_edges) < num_samples:\n",
    "        u, v = np.random.randint(0, N, 2)\n",
    "        if u != v and (u, v) not in pos_set:\n",
    "            neg_edges.append([u, v])\n",
    "    return np.array(neg_edges)\n",
    "\n",
    "\n",
    "# analyze neg sampling\n",
    "sample_negative_edges(pos_edges, N, num_samples=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b950467",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNEncoder(input_dim=features_nb, hidden_dim=32, embedding_dim=16).to(device)\n",
    "z = model(x.to(device), adj_bar.to(device))\n",
    "ic(\n",
    "    z.shape\n",
    ")  # Should be (N, 16), N being number of nodes. As expected, we have a 16-dim embedding for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder: compute logit (dot product) as similarity measure. Like in node2vec or others previously seen.\n",
    "def decode(z, edges):\n",
    "    return torch.sum(z[edges[:, 0]] * z[edges[:, 1]], dim=1)\n",
    "\n",
    "\n",
    "ic(\"There are \", pos_edges.shape[0], \" edges in the graph. -> positive samples.\")\n",
    "ic(pos_edges.shape)\n",
    "pos_edges_tensor = torch.LongTensor(pos_edges).to(device)\n",
    "ic(decode(z, pos_edges_tensor).shape);  # Should be (num_pos,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e637cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alias_edge(G, src, curr, p, q):\n",
    "    \"\"\"Compute transition probabilities for neighbors of 'dst' based on Node2Vec bias.\n",
    "    dst: current node\n",
    "    src: previous node\n",
    "    p: return back to previous node. Lower p means backtracking is more likely.\n",
    "    q: in-out param: explore new nodes (ratio of bfs[breadth-first search] to dfs[depth-first search]).\n",
    "        lower q means more exploration of distant nodes\n",
    "    \"\"\"\n",
    "    unnormalized_probs = []\n",
    "    for neighbor in G[curr]:\n",
    "        if neighbor == src:\n",
    "            weight = 1.0 / p\n",
    "        elif G.has_edge(\n",
    "            neighbor, src\n",
    "        ):  # if neighbor is a neighbor of src (above ex: s1 is neighbor of t)\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            weight = 1.0 / q\n",
    "        unnormalized_probs.append(weight)\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs = [w / norm_const for w in unnormalized_probs]\n",
    "    return list(G[curr]), normalized_probs\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start, walk_length, p, q):\n",
    "    \"\"\"Generate a random walk starting from the given node.\"\"\"\n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        neighbors = list(G.neighbors(cur))\n",
    "        if len(neighbors) == 0:\n",
    "            break\n",
    "        if len(walk) == 1:\n",
    "            next_node = random.choice(neighbors)\n",
    "        else:\n",
    "            prev = walk[-2]\n",
    "            candidates, probs = get_alias_edge(G, src=prev, curr=cur, p=p, q=q)\n",
    "            next_node = random.choices(candidates, weights=probs, k=1)[0]\n",
    "        walk.append(next_node)\n",
    "    return walk\n",
    "\n",
    "\n",
    "def gen_walk_pairs(walk, window_size):\n",
    "    walk_pairs = []\n",
    "    for i, target in enumerate(walk):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(walk), i + window_size + 1)\n",
    "        ic(i, target, start, end)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                ic(\"pair\", target, j, walk[j])\n",
    "                if walk[j] != target:  # Avoid pairs of the same node\n",
    "                    walk_pairs.append((target, walk[j]))\n",
    "\n",
    "    return walk_pairs\n",
    "\n",
    "\n",
    "def generate_random_walks(adj: np.numarray, walk_length=10, num_walks=5):\n",
    "    G = nx.from_numpy_array(adj)\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        nodes = list(G.nodes())\n",
    "        random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walk = node2vec_walk(G, node, walk_length, p=1, q=1)\n",
    "            walks.append(walk)\n",
    "    return walks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "q = 0.2\n",
    "walk_length = 10\n",
    "num_walks = 2\n",
    "walks = generate_random_walks(\n",
    "    adj=adj_bar.numpy(), walk_length=walk_length, num_walks=num_walks\n",
    ")\n",
    "ic(len(walks), walks[:2])\n",
    "\n",
    "ic.disable()\n",
    "pos_pairs = []\n",
    "for walk in walks:\n",
    "    pairs = gen_walk_pairs(walk, window_size=2)\n",
    "    pos_pairs.extend(pairs)\n",
    "pos_pairs_tensor = torch.LongTensor(pos_pairs).to(device)\n",
    "ic(pos_pairs_tensor.shape)\n",
    "num_positive_samples = pos_pairs_tensor.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ef4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative samples (random non-walk pairs)\n",
    "ic.enable()\n",
    "pos_pair_set = set(tuple(p) for p in pos_pairs)\n",
    "ic(len(pos_pair_set))\n",
    "\n",
    "neg_pairs = []\n",
    "num_negative_samples = 0\n",
    "while num_negative_samples < num_positive_samples:\n",
    "    u, v = np.random.randint(0, N, 2)\n",
    "    if u != v and (u, v) not in pos_pair_set:\n",
    "        neg_pairs.append([u, v])\n",
    "        num_negative_samples += 1\n",
    "\n",
    "neg_pairs_tensor = torch.LongTensor(neg_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2169fae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate negative samples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c67e948",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_logits = torch.sum(z[pos_pairs_tensor[:, 0]] * z[pos_pairs_tensor[:, 1]], dim=1)\n",
    "neg_logits = torch.sum(z[neg_pairs_tensor[:, 0]] * z[neg_pairs_tensor[:, 1]], dim=1)\n",
    "ic(pos_logits.shape, pos_logits[:5])\n",
    "ic(neg_logits.shape, neg_logits[:5]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# x, adj_bar, pos_edges, labels, N = load_cora()\n",
    "# data has been loaded above\n",
    "model = GNNEncoder(input_dim=features_nb, hidden_dim=32, embedding_dim=16).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "num_positive_edges = len(pos_edges)\n",
    "# simiar nodes are those connected by an edge. We could use one that are in the same random walk window.\n",
    "pos_edges_tensor = torch.LongTensor(pos_edges).to(device)\n",
    "x = x.to(device)\n",
    "adj_bar = adj_bar.to(device)  # (where adj_bar = D^{-1} A)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model(x, adj_bar)\n",
    "\n",
    "    # Positive logits\n",
    "    # for positive edges, decode similarity should be high (1)\n",
    "    pos_logits = decode(z, pos_edges_tensor)\n",
    "\n",
    "    # Sample negatives (same number as positives)\n",
    "    neg_edges = sample_negative_edges(pos_edges, N, num_samples=num_positive_edges)\n",
    "    neg_edges_tensor = torch.LongTensor(neg_edges).to(device)\n",
    "    # for negative edges, decode similarity should be low (0)\n",
    "    neg_logits = decode(z, neg_edges_tensor)\n",
    "\n",
    "    # Labels: 1 for pos, 0 for neg\n",
    "    pos_labels = torch.ones(num_positive_edges).to(device=device)\n",
    "    neg_labels = torch.zeros(num_positive_edges).to(device=device)\n",
    "\n",
    "    # Loss\n",
    "    loss = binary_cross_entropy_with_logits(\n",
    "        pos_logits, pos_labels\n",
    "    ) + binary_cross_entropy_with_logits(neg_logits, neg_labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# return model, x, adj_bar, labels\n",
    "ic(model, x, adj_bar, labels);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generate embeddings\n",
    "def inference(model, x, adj_bar):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(x, adj_bar)\n",
    "    return z.cpu().numpy()\n",
    "\n",
    "\n",
    "# Visualization: Project to 2D with SVD (manual PCA) and plot (colored by class)\n",
    "def visualize_embeddings(z, labels):\n",
    "    # Manual PCA using SVD\n",
    "    z_mean = np.mean(z, axis=0)\n",
    "    z_centered = z - z_mean\n",
    "    U, S, Vt = np.linalg.svd(z_centered, full_matrices=False)\n",
    "    z_2d = U[:, :2] * S[:2]\n",
    "\n",
    "    colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, lbl in enumerate(unique_labels):\n",
    "        mask = labels == lbl\n",
    "        plt.scatter(\n",
    "            z_2d[mask, 0],\n",
    "            z_2d[mask, 1],\n",
    "            c=colors[i % len(colors)],\n",
    "            label=f\"Class {lbl}\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.title(\"2D Projection of Node Embeddings on Cora (SVD, colored by class)\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Run everything\n",
    "if __name__ == \"__main__\":\n",
    "    # model, x, adj_bar, labels = train()\n",
    "    z = inference(model, x, adj_bar)\n",
    "    visualize_embeddings(z, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371efe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-gnn-py3.12 (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
