{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac9f81",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e35125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import binary_cross_entropy_with_logits\n",
    "import urllib.request\n",
    "import os\n",
    "import tarfile\n",
    "import random\n",
    "from icecream import ic\n",
    "from torch.nn.functional import cross_entropy\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74838096",
   "metadata": {},
   "source": [
    "# Cora dataset\n",
    "\n",
    "https://graphsandnetworks.com/the-cora-dataset/\n",
    "\n",
    "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words.\n",
    "\n",
    "Load cora:\n",
    "\n",
    "- D_inv is $D_{v,v}=Deg(v) = |N(v)|$, Deg is the nb of neighbours that each node has.\n",
    "- adj_bar will be reused in $H^{(k+1)}=D^{-1}AH^{(k)}$. We will multiply by H at each layer.\n",
    "- x: the features (words within doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d5468f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Cora dataset\n",
    "\n",
    "# Download and extract if not present\n",
    "if not os.path.exists(\"cora\"):\n",
    "    print(\"Downloading Cora dataset...\")\n",
    "    urllib.request.urlretrieve(\n",
    "        \"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\", \"cora.tgz\"\n",
    "    )\n",
    "    with tarfile.open(\"cora.tgz\", \"r:gz\") as tar:\n",
    "        tar.extractall()\n",
    "\n",
    "# Load node features and labels\n",
    "content_path = \"cora/cora.content\"\n",
    "content = np.genfromtxt(content_path, dtype=str)\n",
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "435f73a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ff713e",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_ids = content[:, 0].astype(int)\n",
    "features = content[:, 1:-1].astype(np.float32)\n",
    "class_strs = content[:, -1]\n",
    "\n",
    "N = len(node_ids)\n",
    "node_map = {node_id: i for i, node_id in enumerate(node_ids)}\n",
    "\n",
    "# Features matrix\n",
    "X = features  # Already in shape (N, 1433)\n",
    "\n",
    "# Labels: 7 classes\n",
    "class_map = {\n",
    "    \"Case_Based\": 0,\n",
    "    \"Genetic_Algorithms\": 1,\n",
    "    \"Neural_Networks\": 2,\n",
    "    \"Probabilistic_Methods\": 3,\n",
    "    \"Reinforcement_Learning\": 4,\n",
    "    \"Rule_Learning\": 5,\n",
    "    \"Theory\": 6,\n",
    "}\n",
    "labels = np.array([class_map[c] for c in class_strs])\n",
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9207c1cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "node_map[35]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2991a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load edges (directed citations)\n",
    "cites_path = \"cora/cora.cites\"\n",
    "cites = np.genfromtxt(cites_path, dtype=int)\n",
    "cites\n",
    "\n",
    "edges = []\n",
    "for src, dst in cites:\n",
    "    if src in node_map and dst in node_map:  # Ensure both exist\n",
    "        src_idx = node_map[src]\n",
    "        dst_idx = node_map[dst]\n",
    "        edges.append([src_idx, dst_idx])\n",
    "edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d21a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjacency matrix (undirected, no self-loops)\n",
    "A = np.zeros((N, N), dtype=np.float32)\n",
    "for u, v in edges:\n",
    "    A[u, v] = 1.0\n",
    "    # A[v, u] = 1.0  # Make undirected\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd593c9",
   "metadata": {},
   "source": [
    "# try smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe1c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.eye(N)\n",
    "A_tilde = A + np.identity(N)\n",
    "D = np.sum(A_tilde, axis=1)\n",
    "D\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7110f330",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_inv_sqrt = np.power(D, -0.5)\n",
    "D_inv_sqrt = np.where(np.isinf(D_inv_sqrt), 0.0, D_inv_sqrt)\n",
    "D_inv_sqrt = np.diag(D_inv_sqrt)\n",
    "A_norm = D_inv_sqrt @ A_tilde @ D_inv_sqrt\n",
    "\n",
    "A_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0be1c4",
   "metadata": {},
   "source": [
    "   A_tilde = A + I  # Add identity matrix\n",
    "\n",
    "   D_tilde = degree_matrix(A_tilde)  # Now all nodes have degree â‰¥ 1\n",
    "   \n",
    "   D_inv = D_tilde^{-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c10b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see page 50 of 03-GNN1.pdf\n",
    "# Laplacian normalization D^-1 A\n",
    "\n",
    "degrees = np.sum(A, axis=1)\n",
    "degrees[degrees == 0] = 1.0\n",
    "# degrees = degrees + 1\n",
    "degrees\n",
    "# Avoid division by zero\n",
    "D_inv = np.diag(1.0 / (degrees))\n",
    "D_inv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791addf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adj_bar will be used in $$D^{-1}AH^{(k)}$$\n",
    "\n",
    "adj_bar = D_inv @ A\n",
    "adj_bar = torch.FloatTensor(adj_bar)\n",
    "adj_bar.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df97d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.FloatTensor(X)\n",
    "pos_edges = np.array(edges)  # Directed for positive samples\n",
    "pos_edges\n",
    "\n",
    "# x, adj_bar, pos_edges, labels, N = load_cora()\n",
    "ic(x.shape, adj_bar.shape, pos_edges.shape, labels.shape, degrees.shape, N, set(labels))\n",
    "features_nb = x.shape[1]\n",
    "ic(features_nb);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501d4d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees\n",
    "np.percentile(degrees, 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3432ba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network visualization of a subset of the graph\n",
    "import networkx as nx\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "# Class distribution\n",
    "class_names = [\n",
    "    \"Case_Based\",\n",
    "    \"Genetic_Algorithms\",\n",
    "    \"Neural_Networks\",\n",
    "    \"Probabilistic_Methods\",\n",
    "    \"Reinforcement_Learning\",\n",
    "    \"Rule_Learning\",\n",
    "    \"Theory\",\n",
    "]\n",
    "\n",
    "# Create a NetworkX graph from a subset of nodes (for visualization purposes)\n",
    "# Using only nodes with high degree to keep visualization manageable\n",
    "# threshold  of degree where node degree is in the top 10%\n",
    "high_degree_threshold = np.percentile(degrees, 80)  # Top 10% by degree\n",
    "\n",
    "high_degree_nodes = np.where(degrees >= high_degree_threshold)[0]\n",
    "\n",
    "print(\n",
    "    f\"Visualizing subgraph with {len(high_degree_nodes)} high-degree nodes (degree >= {high_degree_threshold:.0f})\"\n",
    ")\n",
    "\n",
    "# Create subgraph\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(high_degree_nodes)\n",
    "\n",
    "# Add edges between high-degree nodes\n",
    "for u, v in pos_edges:\n",
    "    if u in high_degree_nodes and v in high_degree_nodes:\n",
    "        G.add_edge(u, v)\n",
    "\n",
    "print(f\"Subgraph has {G.number_of_nodes()} nodes and {G.number_of_edges()} edges\")\n",
    "\n",
    "# Create node colors based on class labels\n",
    "node_colors = [labels[node] for node in G.nodes()]\n",
    "colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "pos = nx.spring_layout(G, k=1, iterations=50)\n",
    "\n",
    "nx.draw(\n",
    "    G,\n",
    "    pos,\n",
    "    node_color=[colors[c] for c in node_colors],\n",
    "    node_size=50,\n",
    "    edge_color=\"gray\",\n",
    "    alpha=0.8,\n",
    "    with_labels=False,\n",
    ")\n",
    "\n",
    "# Create legend\n",
    "\n",
    "legend_elements = [\n",
    "    Patch(facecolor=colors[i], label=class_names[i]) for i in range(len(class_names))\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
    "plt.title(\n",
    "    \"Cora Citation Network (High-degree nodes subgraph)\\nColored by Publication Class\"\n",
    ")\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e637cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_alias_edge(G, src, curr, p, q):\n",
    "    \"\"\"Compute transition probabilities for neighbors of 'dst' based on Node2Vec bias.\n",
    "    dst: current node\n",
    "    src: previous node\n",
    "    p: return back to previous node. Lower p means backtracking is more likely.\n",
    "    q: in-out param: explore new nodes (ratio of bfs[breadth-first search] to dfs[depth-first search]).\n",
    "        lower q means more exploration of distant nodes\n",
    "    \"\"\"\n",
    "    unnormalized_probs = []\n",
    "    for neighbor in G[curr]:\n",
    "        if neighbor == src:\n",
    "            weight = 1.0 / p\n",
    "        elif G.has_edge(\n",
    "            neighbor, src\n",
    "        ):  # if neighbor is a neighbor of src (above ex: s1 is neighbor of t)\n",
    "            weight = 1.0\n",
    "        else:\n",
    "            weight = 1.0 / q\n",
    "        unnormalized_probs.append(weight)\n",
    "    norm_const = sum(unnormalized_probs)\n",
    "    normalized_probs = [w / norm_const for w in unnormalized_probs]\n",
    "    return list(G[curr]), normalized_probs\n",
    "\n",
    "\n",
    "def node2vec_walk(G, start, walk_length, p, q):\n",
    "    \"\"\"Generate a random walk starting from the given node.\"\"\"\n",
    "    walk = [start]\n",
    "    while len(walk) < walk_length:\n",
    "        cur = walk[-1]\n",
    "        neighbors = list(G.neighbors(cur))\n",
    "        if len(neighbors) == 0:\n",
    "            break\n",
    "        if len(walk) == 1:\n",
    "            next_node = random.choice(neighbors)\n",
    "        else:\n",
    "            prev = walk[-2]\n",
    "            candidates, probs = get_alias_edge(G, src=prev, curr=cur, p=p, q=q)\n",
    "            next_node = random.choices(candidates, weights=probs, k=1)[0]\n",
    "        walk.append(next_node)\n",
    "    return walk\n",
    "\n",
    "\n",
    "def generate_random_walks(adj: np.numarray, walk_length=10, num_walks=5, p=1, q=1):\n",
    "    G = nx.from_numpy_array(adj)\n",
    "    walks = []\n",
    "    for _ in range(num_walks):\n",
    "        nodes = list(G.nodes())\n",
    "        random.shuffle(nodes)\n",
    "        for node in nodes:\n",
    "            walk = node2vec_walk(G, node, walk_length, p=p, q=q)\n",
    "            walks.append(walk)\n",
    "    return walks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04aceb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 1\n",
    "q = 0.2\n",
    "walk_length = 10\n",
    "num_walks = 2\n",
    "walks = generate_random_walks(\n",
    "    adj=adj_bar.numpy(), walk_length=walk_length, num_walks=num_walks, p=p, q=q\n",
    ")\n",
    "len(walks)\n",
    "\n",
    "# walks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abde2d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic(len(walks), walks[0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e42b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_walk_pairs(walk, window_size):\n",
    "    walk_pairs = []\n",
    "    for i, target in enumerate(walk):\n",
    "        start = max(0, i - window_size)\n",
    "        end = min(len(walk), i + window_size + 1)\n",
    "        ic(i, target, start, end)\n",
    "        for j in range(start, end):\n",
    "            if i != j:\n",
    "                ic(\"pair\", target, j, walk[j])\n",
    "                if walk[j] != target:  # Avoid pairs of the same node\n",
    "                    walk_pairs.append((target, walk[j]))\n",
    "\n",
    "    return walk_pairs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4060d7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ic.enable()\n",
    "ic.disable()\n",
    "pos_pairs = []\n",
    "for walk in walks[:]:\n",
    "    pairs = gen_walk_pairs(walk, window_size=2)\n",
    "    pos_pairs.extend(pairs)\n",
    "\n",
    "num_positive_samples = len(pos_pairs)\n",
    "ic(num_positive_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fb0087",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pos_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974ef4ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative samples (random non-walk pairs)\n",
    "ic.enable()\n",
    "pos_pair_set = set(tuple(p) for p in pos_pairs)\n",
    "ic(len(pos_pair_set))\n",
    "\n",
    "neg_pairs = []\n",
    "num_negative_samples = 0\n",
    "while num_negative_samples < num_positive_samples:\n",
    "    u, v = np.random.randint(0, N, 2)\n",
    "    if u != v and (u, v) not in pos_pair_set:\n",
    "        neg_pairs.append([u, v])\n",
    "        num_negative_samples += 1\n",
    "\n",
    "ic(len(neg_pairs), neg_pairs[:5]);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bd7995",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c90238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GNN Layer (with separate W for neighbors and B for self)\n",
    "class GNNLayer(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        super(GNNLayer, self).__init__()\n",
    "        self.W = nn.Parameter(torch.FloatTensor(in_dim, out_dim))\n",
    "        self.B = nn.Parameter(torch.FloatTensor(in_dim, out_dim))\n",
    "        nn.init.xavier_uniform_(self.W)\n",
    "        nn.init.xavier_uniform_(self.B)\n",
    "\n",
    "    def forward(self, h, adj_bar):\n",
    "        # Neighbor aggregation: adj_bar @ h @ W (where adj_bar = D^{-1} A).\n",
    "        # adj_bar stands for adjusted adjacency matrix.\n",
    "        # torch.mm is optimized for 2d matrix operations: faster, less memory,..\n",
    "        neigh_h = torch.mm(adj_bar, h)\n",
    "        neigh_h = torch.mm(neigh_h, self.W)\n",
    "        # Self transformation: h @ B\n",
    "        self_h = torch.mm(h, self.B)\n",
    "        # Add them\n",
    "        return neigh_h + self_h\n",
    "\n",
    "\n",
    "# GNN Encoder\n",
    "class GNNEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim):\n",
    "        super(GNNEncoder, self).__init__()\n",
    "        self.layer1 = GNNLayer(\n",
    "            input_dim, hidden_dim\n",
    "        )  # Map features size to hidden size\n",
    "        self.layer2 = GNNLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GNNLayer(hidden_dim, embedding_dim)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj_bar):\n",
    "        h = self.layer1(x, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer2(h, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer3(h, adj_bar)  # No activation on last layer for embeddings\n",
    "        return h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b950467",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNEncoder(input_dim=features_nb, hidden_dim=32, embedding_dim=16).to(device)\n",
    "z = model(x.to(device), adj_bar.to(device))\n",
    "ic(\n",
    "    z.shape\n",
    ")  # Should be (N, 16), N being number of nodes. As expected, we have a 16-dim embedding for each node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9479f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder: compute logit (dot product) as similarity measure. Like in node2vec or others previously seen.\n",
    "def decode(z, edges):\n",
    "    return torch.sum(z[edges[:, 0]] * z[edges[:, 1]], dim=1)\n",
    "\n",
    "\n",
    "ic(\"There are \", pos_edges.shape[0], \" edges in the graph. -> positive samples.\")\n",
    "ic(pos_edges.shape)\n",
    "pos_edges_tensor = torch.LongTensor(pos_edges).to(device)\n",
    "ic(decode(z, pos_edges_tensor).shape);  # Should be (num_pos,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750e7b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "\n",
    "# x, adj_bar, pos_edges, labels, N = load_cora()\n",
    "# data has been loaded above\n",
    "model = GNNEncoder(input_dim=features_nb, hidden_dim=32, embedding_dim=16).to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "# Use randow walk\n",
    "num_positive_pairs = len(pos_pairs)\n",
    "num_negative_pairs = len(neg_pairs)\n",
    "ic(num_positive_pairs, num_negative_pairs)\n",
    "neg_pairs_tensor = torch.LongTensor(neg_pairs).to(device)\n",
    "pos_pairs_tensor = torch.LongTensor(pos_pairs).to(device)\n",
    "\n",
    "x = x.to(device)\n",
    "adj_bar = adj_bar.to(device)  # (where adj_bar = D^{-1} A)\n",
    "# labels = labels.to(device)\n",
    "\n",
    "for epoch in range(500):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    z = model(x, adj_bar)\n",
    "\n",
    "    # Positive logits\n",
    "    # for positive edges, decode similarity should be high (1)\n",
    "    pos_logits = decode(z, pos_pairs_tensor)\n",
    "\n",
    "    # for negative edges, decode similarity should be low (0)\n",
    "    neg_logits = decode(z, neg_pairs_tensor)\n",
    "\n",
    "    # Labels: 1 for pos, 0 for neg\n",
    "    pos_labels = torch.ones(num_positive_pairs).to(device=device)\n",
    "    neg_labels = torch.zeros(num_negative_pairs).to(device=device)\n",
    "\n",
    "    # Loss\n",
    "    loss = binary_cross_entropy_with_logits(\n",
    "        pos_logits, pos_labels\n",
    "    ) + binary_cross_entropy_with_logits(neg_logits, neg_labels)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# return model, x, adj_bar, labels\n",
    "ic(model, x, adj_bar, labels);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ab9c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inference: Generate embeddings\n",
    "def inference(model, x, adj_bar):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        z = model(x, adj_bar)\n",
    "    return z.cpu().numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d371efe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "def visualize_tsne_embeddings(z, labels):\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30, max_iter=1000)\n",
    "    z_2d = tsne.fit_transform(z)\n",
    "\n",
    "    colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    for i, lbl in enumerate(unique_labels):\n",
    "        mask = labels == lbl\n",
    "        plt.scatter(\n",
    "            z_2d[mask, 0],\n",
    "            z_2d[mask, 1],\n",
    "            c=colors[i % len(colors)],\n",
    "            label=f\"Class {lbl}\",\n",
    "            alpha=0.7,\n",
    "        )\n",
    "    plt.legend()\n",
    "    plt.title(\"2D Projection of Node Embeddings on Cora (t-SNE, colored by class)\")\n",
    "    plt.xlabel(\"t-SNE Component 1\")\n",
    "    plt.ylabel(\"t-SNE Component 2\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "z = inference(model, x, adj_bar)\n",
    "visualize_tsne_embeddings(z, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e698b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "\n",
    "def visualize_tsne_embeddings_3d(z, labels):\n",
    "    tsne = TSNE(n_components=3, random_state=42, perplexity=30, max_iter=1000)\n",
    "    z_3d = tsne.fit_transform(z)\n",
    "\n",
    "    colors = [\"red\", \"blue\", \"green\", \"orange\", \"purple\", \"brown\", \"pink\"]\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection=\"3d\")\n",
    "\n",
    "    for i, lbl in enumerate(unique_labels):\n",
    "        mask = labels == lbl\n",
    "        ax.scatter(\n",
    "            z_3d[mask, 0],\n",
    "            z_3d[mask, 1],\n",
    "            z_3d[mask, 2],\n",
    "            c=colors[i % len(colors)],\n",
    "            label=f\"Class {lbl}\",\n",
    "            alpha=0.7,\n",
    "            s=20,  # point size\n",
    "        )\n",
    "\n",
    "    ax.legend()\n",
    "    ax.set_title(\"3D Projection of Node Embeddings on Cora (t-SNE, colored by class)\")\n",
    "    ax.set_xlabel(\"t-SNE Component 1\")\n",
    "    ax.set_ylabel(\"t-SNE Component 2\")\n",
    "    ax.set_zlabel(\"t-SNE Component 3\")\n",
    "\n",
    "    # Enable rotation\n",
    "    ax.view_init(elev=20, azim=45)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Generate 3D t-SNE visualization\n",
    "visualize_tsne_embeddings_3d(z, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74082c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "\n",
    "\n",
    "def visualize_embeddings_tensorboard(z, labels, class_names, log_dir=\"runs/embeddings\"):\n",
    "    \"\"\"\n",
    "    Visualize embeddings in TensorBoard with interactive 3D projection\n",
    "    \"\"\"\n",
    "    # Create log directory\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "    # Initialize TensorBoard writer\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    # Convert to torch tensor if needed\n",
    "    if isinstance(z, np.ndarray):\n",
    "        embeddings = torch.FloatTensor(z)\n",
    "    else:\n",
    "        embeddings = z\n",
    "\n",
    "    # Create metadata (class labels)\n",
    "    metadata = [class_names[label] for label in labels]\n",
    "\n",
    "    # Add embeddings to TensorBoard\n",
    "    writer.add_embedding(embeddings, metadata=metadata, tag=\"Node_Embeddings\")\n",
    "\n",
    "    writer.close()\n",
    "    print(f\"Embeddings saved to TensorBoard. Run the following command to view:\")\n",
    "    print(f\"tensorboard --logdir=nbks/{log_dir}\")\n",
    "    print(\"Then open your browser to http://localhost:6006\")\n",
    "    print(\"Navigate to the 'Projector' tab to see the interactive 3D visualization\")\n",
    "\n",
    "\n",
    "# Generate TensorBoard visualization\n",
    "class_names = [\n",
    "    \"Case_Based\",\n",
    "    \"Genetic_Algorithms\",\n",
    "    \"Neural_Networks\",\n",
    "    \"Probabilistic_Methods\",\n",
    "    \"Reinforcement_Learning\",\n",
    "    \"Rule_Learning\",\n",
    "    \"Theory\",\n",
    "]\n",
    "\n",
    "visualize_embeddings_tensorboard(z, labels, class_names)\n",
    "\n",
    "# uv run tensorboard --logdir nbks/runs/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e366e2f",
   "metadata": {},
   "source": [
    "# use classification instead of similarity as target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea005bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GNNClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, embedding_dim, num_classes):\n",
    "        super(GNNClassifier, self).__init__()\n",
    "        self.layer1 = GNNLayer(input_dim, hidden_dim)\n",
    "        self.layer2 = GNNLayer(hidden_dim, hidden_dim)\n",
    "        self.layer3 = GNNLayer(hidden_dim, embedding_dim)\n",
    "        self.classifier = nn.Linear(embedding_dim, num_classes)\n",
    "        self.activation = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, adj_bar):\n",
    "        h = self.layer1(x, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer2(h, adj_bar)\n",
    "        h = self.activation(h)\n",
    "        h = self.layer3(h, adj_bar)  # Embeddings\n",
    "        logits = self.classifier(h)  # Classification logits\n",
    "        return h, logits  # Return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d67341b",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_classes = len(set(labels.tolist()))\n",
    "y = torch.LongTensor(labels).to(device)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f612e028",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNClassifier(\n",
    "    input_dim=features_nb, hidden_dim=32, embedding_dim=16, num_classes=nb_classes\n",
    ").to(device)\n",
    "h, train_logits = model(x.to(device), adj_bar.to(device))\n",
    "ic(h.shape, train_logits.shape)  # h: (N, 16), logits: (N, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "403ed99d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GNNClassifier(\n",
    "    input_dim=1433, hidden_dim=32, embedding_dim=16, num_classes=7\n",
    ").to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Split into train/val/test (60%/20%/20%)\n",
    "idx = np.arange(N)\n",
    "train_idx, test_idx = train_test_split(idx, test_size=0.4, random_state=42)\n",
    "val_idx, test_idx = train_test_split(test_idx, test_size=0.5, random_state=42)\n",
    "train_idx = torch.LongTensor(train_idx).to(device)\n",
    "val_idx = torch.LongTensor(val_idx).to(device)\n",
    "test_idx = torch.LongTensor(test_idx).to(device)\n",
    "\n",
    "# Subset train data\n",
    "train_x = x[train_idx]\n",
    "train_adj = adj_bar[train_idx][:, train_idx]  # Subgraph adjacency\n",
    "train_y = y[train_idx]\n",
    "train_labels = labels[train_idx.cpu().numpy()]\n",
    "\n",
    "val_x = x[val_idx]\n",
    "val_adj = adj_bar[val_idx][:, val_idx]  # Subgraph adjacency\n",
    "val_y = y[val_idx]\n",
    "val_labels = labels[val_idx.cpu().numpy()]\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "for epoch in range(200):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    train_embeddings, train_logits = model(train_x, train_adj)  # Train on subgraph\n",
    "\n",
    "    # Compute loss on training set\n",
    "    train_loss = cross_entropy(train_logits, train_y)\n",
    "\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Validation on full graph\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        val_embeddings, val_logits = model(\n",
    "            val_x, val_adj\n",
    "        )  # Use full graph for embeddings\n",
    "        val_loss = cross_entropy(val_logits, val_y)\n",
    "\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_model = model.state_dict()\n",
    "\n",
    "    if epoch % 20 == 0:\n",
    "        print(\n",
    "            f\"Epoch {epoch}, Train Loss: {train_loss.item():.4f}, Val Loss: {val_loss.item():.4f}\"\n",
    "        )\n",
    "\n",
    "# Load best model and generate full embeddings\n",
    "model.load_state_dict(best_model)\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    full_embeddings, _ = model(x, adj_bar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5d68ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne_embeddings(train_embeddings.detach().cpu().numpy(), train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d102899",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne_embeddings(val_embeddings.cpu().numpy(), val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f88b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_tsne_embeddings(full_embeddings.cpu().numpy(), labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f1296b8",
   "metadata": {},
   "source": [
    "# GraphSage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd0cbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03093b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Map original paper IDs to contiguous integer indices (0 to N-1)\n",
    "paper_ids = content[:, 0]\n",
    "id_to_idx = {int(p_id): i for i, p_id in enumerate(paper_ids)}\n",
    "N = len(paper_ids)  # Number of nodes\n",
    "\n",
    "# 2. Extract Features (Bag-of-Words)\n",
    "features = torch.tensor(content[:, 1:-1].astype(np.float32)).to(device)\n",
    "D = features.size(1)  # Feature dimension\n",
    "\n",
    "# 3. Extract and Encode Labels\n",
    "labels_text = content[:, -1]\n",
    "label_map = {name: i for i, name in enumerate(np.unique(labels_text))}\n",
    "labels = np.array([label_map[name] for name in labels_text])\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "C = len(label_map)  # Number of classes\n",
    "class_names = list(label_map.keys())  # For visualization legend\n",
    "\n",
    "# 4. Process Edges and Symmetrize\n",
    "source_nodes = np.array([id_to_idx[src] for src in cites[:, 0]])\n",
    "target_nodes = np.array([id_to_idx[tgt] for tgt in cites[:, 1]])\n",
    "\n",
    "# Ensure tensors for stacking\n",
    "source_tensor = torch.tensor(source_nodes, dtype=torch.long)\n",
    "target_tensor = torch.tensor(target_nodes, dtype=torch.long)\n",
    "\n",
    "edge_index = torch.stack([source_tensor, target_tensor], dim=0)\n",
    "\n",
    "# Symmetrize edges\n",
    "reversed_edges = torch.stack([target_tensor, source_tensor], dim=0)\n",
    "edge_index = torch.cat([edge_index, reversed_edges], dim=1)\n",
    "edge_index = torch.unique(edge_index, dim=1)\n",
    "\n",
    "\n",
    "# 5. Create Adjacency Matrix and Normalized Adjacency (D^{-1}A)\n",
    "def create_normalized_adj(edge_index, num_nodes, device):\n",
    "    # Create the adjacency matrix (A)\n",
    "    adj = torch.zeros(num_nodes, num_nodes, dtype=torch.float, device=device)\n",
    "    adj[edge_index[0].to(device), edge_index[1].to(device)] = 1.0\n",
    "\n",
    "    # Add self-loops (A_hat = A + I) for including self-feature in aggregation\n",
    "    A_hat = adj + torch.eye(num_nodes, device=device)\n",
    "\n",
    "    # Calculate degree matrix (D_hat) for Mean Aggregation: D_ii = Sum_j (A_hat_ij)\n",
    "    D_hat_inv = torch.diag(1.0 / A_hat.sum(dim=1))\n",
    "\n",
    "    # D_hat_inv @ A_hat is the normalized matrix where each row sums to 1 (Mean Aggregation)\n",
    "    A_norm = torch.matmul(D_hat_inv, A_hat)\n",
    "    return A_norm\n",
    "\n",
    "\n",
    "# Create the normalized adjacency matrix for aggregation\n",
    "A_norm = create_normalized_adj(edge_index, N, device)\n",
    "\n",
    "\n",
    "# 6. Create Standard Planetoid Masks\n",
    "train_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "val_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "test_mask = torch.zeros(N, dtype=torch.bool, device=device)\n",
    "\n",
    "train_mask[:140] = True\n",
    "val_mask[140 : 140 + 500] = True\n",
    "test_mask[140 + 500 : 140 + 500 + 1000] = True\n",
    "\n",
    "\n",
    "# --- 2. Pure PyTorch GraphSAGE Implementation ---\n",
    "\n",
    "\n",
    "class SageConv(nn.Module):\n",
    "    \"\"\"\n",
    "    GraphSAGE Convolutional Layer (Mean Aggregation) in Pure PyTorch.\n",
    "\n",
    "    The update rule is h'_u = W * CONCAT(h_u, MEAN({h_v | v in N(u)}))\n",
    "    In matrix form: H' = A_norm @ H @ W_agg + H @ W_self\n",
    "    Here, we combine W_agg and W_self into a single CONCAT and W matrix for efficiency.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        # Since we aggregate neighbors (in_features) and concatenate the node's own feature (in_features),\n",
    "        # the input dimension to the final linear layer is 2 * in_features.\n",
    "        self.linear = nn.Linear(2 * in_features, out_features)\n",
    "\n",
    "    def forward(self, x, A_norm):\n",
    "        \"\"\"\n",
    "        x: Node features (N, D_in)\n",
    "        A_norm: Normalized Adjacency Matrix (N, N) where A_norm[i, j] is 1/|N(i)| if j is a neighbor.\n",
    "        \"\"\"\n",
    "        # 1. Neighborhood Aggregation (Mean)\n",
    "        # H_agg = A_norm @ X  -> (N, N) x (N, D_in) = (N, D_in)\n",
    "        # This computes the average of neighbor features (including self-loop)\n",
    "        x_aggregated = torch.matmul(A_norm, x)\n",
    "\n",
    "        # 2. Concatenation\n",
    "        # CONCAT(X_self, X_aggregated) -> (N, 2 * D_in)\n",
    "        x_combined = torch.cat([x, x_aggregated], dim=1)\n",
    "\n",
    "        # 3. Projection and Activation\n",
    "        # (N, 2 * D_in) @ W -> (N, D_out)\n",
    "        output = self.linear(x_combined)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class GraphSAGE(nn.Module):\n",
    "    \"\"\"\n",
    "    The complete GraphSAGE Model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, hidden_dim, out_classes, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1: in_features -> hidden_dim\n",
    "        self.conv1 = SageConv(in_features, hidden_dim)\n",
    "        self.bn1 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Layer 2: hidden_dim -> hidden_dim\n",
    "        self.conv2 = SageConv(hidden_dim, hidden_dim)\n",
    "        self.bn2 = nn.BatchNorm1d(hidden_dim)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "\n",
    "        # Final classification layer: hidden_dim -> out_classes\n",
    "        self.output_proj = nn.Linear(hidden_dim, out_classes)\n",
    "\n",
    "    def forward(self, x, A_norm):\n",
    "        # Layer 1\n",
    "        x = self.conv1(x, A_norm)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Layer 2\n",
    "        x = self.conv2(x, A_norm)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Store final embeddings before classifier\n",
    "        final_embeddings = x\n",
    "\n",
    "        # Classification\n",
    "        classification_output = self.output_proj(x)\n",
    "\n",
    "        # Return both the classification output and the hidden embeddings\n",
    "        return F.log_softmax(classification_output, dim=1), final_embeddings\n",
    "\n",
    "\n",
    "# --- 3. Model Initialization and Training ---\n",
    "\n",
    "# Hyperparameters (Adjusted for GraphSAGE common practice)\n",
    "HIDDEN_DIM = 128  # Typically wider layers than attention for SAGE\n",
    "DROPOUT = 0.5\n",
    "LR = 0.01\n",
    "EPOCHS = 200\n",
    "\n",
    "# Initialize Model and move it to the selected device\n",
    "model = GraphSAGE(\n",
    "    in_features=D, hidden_dim=HIDDEN_DIM, out_classes=C, dropout_rate=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LR, weight_decay=5e-4)\n",
    "\n",
    "\n",
    "# Training function\n",
    "def train(model, x, A_norm, y, train_mask, optimizer):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    # out[0] is classification output, out[1] is embeddings\n",
    "    out, _ = model(x, A_norm)\n",
    "    loss = F.nll_loss(out[train_mask], y[train_mask])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "# Evaluation function\n",
    "@torch.no_grad()\n",
    "def test(model, x, A_norm, y, test_mask):\n",
    "    model.eval()\n",
    "    out, _ = model(x, A_norm)\n",
    "    pred = out.argmax(dim=1)\n",
    "    correct = pred[test_mask].eq(y[test_mask]).sum().item()\n",
    "    acc = correct / test_mask.sum().item()\n",
    "    return acc\n",
    "\n",
    "\n",
    "# Visualization function (identical to previous file)\n",
    "def visualize_embeddings(embeddings, labels, class_names, title):\n",
    "    print(\n",
    "        f\"\\n--- Running t-SNE for Visualization (N={embeddings.shape[0]}, D={embeddings.shape[1]}) ---\"\n",
    "    )\n",
    "\n",
    "    tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "    embeddings_2d = tsne.fit_transform(embeddings)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "    colors = plt.cm.get_cmap(\"viridis\", len(unique_labels))\n",
    "\n",
    "    for i, label in enumerate(unique_labels):\n",
    "        indices = labels == label\n",
    "        plt.scatter(\n",
    "            embeddings_2d[indices, 0],\n",
    "            embeddings_2d[indices, 1],\n",
    "            c=[colors(i)],\n",
    "            label=class_names[label],\n",
    "            s=20,\n",
    "            alpha=0.7,\n",
    "            edgecolors=\"w\",\n",
    "            linewidths=0.5,\n",
    "        )\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(title=\"Research Area\")\n",
    "    plt.xlabel(\"t-SNE Dimension 1\")\n",
    "    plt.ylabel(\"t-SNE Dimension 2\")\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- 4. Main Execution and Visualization ---\n",
    "\n",
    "print(f\"Starting training on Cora dataset (Nodes: {N}, Features: {D}, Classes: {C})...\")\n",
    "best_val_acc = 0.0\n",
    "best_test_acc = 0.0\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    loss = train(model, features, A_norm, labels, train_mask, optimizer)\n",
    "    val_acc = test(model, features, A_norm, labels, val_mask)\n",
    "    current_test_acc = test(model, features, A_norm, labels, test_mask)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        best_test_acc = current_test_acc\n",
    "\n",
    "    if epoch % 50 == 0 or epoch == EPOCHS:\n",
    "        print(\n",
    "            f\"Epoch: {epoch:03d}, Loss: {loss:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {current_test_acc:.4f}\"\n",
    "        )\n",
    "\n",
    "print(f\"\\n--- Training Complete ---\")\n",
    "print(f\"Best Test Accuracy (based on max validation accuracy): **{best_test_acc:.4f}**\")\n",
    "\n",
    "# Get the final embeddings and labels for visualization\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, final_embeddings_tensor = model(features, A_norm)\n",
    "\n",
    "# Move tensors back to CPU and convert to numpy for t-SNE and Matplotlib\n",
    "final_embeddings_np = final_embeddings_tensor.cpu().numpy()\n",
    "labels_np = labels.cpu().numpy()\n",
    "\n",
    "# Run Visualization\n",
    "visualize_embeddings(\n",
    "    final_embeddings_np,\n",
    "    labels_np,\n",
    "    class_names,\n",
    "    \"t-SNE Visualization of Node Embeddings (GraphSAGE on Cora)\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098175a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cop-gnn-py3.12 (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
